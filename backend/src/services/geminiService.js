import { GoogleGenerativeAI } from '@google/generative-ai';
import { GoogleAICacheManager } from '@google/generative-ai/server';
import fs from 'fs';
import path from 'path';
import crypto from 'crypto';
import { createRequire } from 'module';
import logger from '../config/logger.js';

// Usar createRequire para carregar pdf-parse (CommonJS) em ES module
const require = createRequire(import.meta.url);
let pdfParse;

// Cache de hist√≥rico por n√∫mero de telefone
const userConversations = new Map();

// Configura√ß√µes fixas do sistema
const FIXED_MODEL = 'gemini-2.5-flash';
const FIXED_TEMPERATURE = 1.0;
const CACHE_TTL_MINUTES = process.env.GEMINI_CACHE_TTL ? parseInt(process.env.GEMINI_CACHE_TTL) : 60; // Tempo de vida do cache em minutos

// Cache local para rastrear caches criados no Gemini (hash -> { name, expireTime })
const systemPromptCache = new Map();

// Diretrizes fixas que SEMPRE ser√£o aplicadas
const SYSTEM_GUIDELINES = `
Diretrizes:
- Seja sempre educado e respeitoso
- Forne√ßa respostas precisas e √∫teis
- Se n√£o souber algo, admita honestamente
- Adapte seu tom ao contexto da conversa
- Mantenha as respostas concisas quando poss√≠vel
`;

/**
 * Combina o prompt personalizado do usu√°rio com as diretrizes fixas do sistema
 */
function buildSystemPrompt(customPrompt = '') {
  if (customPrompt && customPrompt.trim()) {
    return `${customPrompt.trim()}\n\n${SYSTEM_GUIDELINES}`;
  }
  return `Voc√™ √© um assistente virtual prestativo e profissional.\n${SYSTEM_GUIDELINES}`;
}

/**
 * Obt√©m ou cria um cache de contexto para o prompt do sistema
 */
async function getOrCreateCache(apiKey, systemPrompt) {
  try {
    // Se o prompt for muito curto, n√£o vale a pena (ou a API rejeita) fazer cache
    // O limite oficial √© ~32k tokens, mas vamos tentar para prompts maiores que 1000 chars por enquanto
    // ou deixar a API decidir e tratar o erro.
    // Para este caso, vamos tentar cachear se tiver mais de 100 caracteres para testar,
    // mas sabendo que a API pode rejeitar se for muito pequeno (depende do modelo).
    // O usu√°rio pediu para implementar, ent√£o vamos tentar.

    const hash = crypto.createHash('md5').update(systemPrompt).digest('hex');
    const now = Date.now();
    const cacheDisplayName = `sys_prompt_${hash.substring(0, 8)}`;

    // 1. Verificar se j√° temos um cache v√°lido localmente (Mem√≥ria RAM)
    if (systemPromptCache.has(hash)) {
      const cached = systemPromptCache.get(hash);
      // Margem de seguran√ßa de 5 minutos antes de expirar
      if (cached.expireTime > now + 5 * 60 * 1000) {
        logger.info(`üì¶ Usando cache de contexto existente (Mem√≥ria): ${cached.name}`);
        return { name: cached.name };
      } else {
        logger.info(`üì¶ Cache local expirado ou pr√≥ximo de expirar: ${cached.name}`);
        systemPromptCache.delete(hash);
      }
    }

    const cacheManager = new GoogleAICacheManager(apiKey);

    // 2. Verificar se j√° existe um cache v√°lido no Servidor do Google (Persist√™ncia entre restarts)
    try {
      logger.info('üîç Verificando caches existentes no servidor Gemini...');
      const listResult = await cacheManager.list();

      if (listResult.cachedContents) {
        const existingCache = listResult.cachedContents.find(c =>
          c.displayName === cacheDisplayName &&
          new Date(c.expireTime).getTime() > now + 5 * 60 * 1000 // Verifica se ainda √© v√°lido
        );

        if (existingCache) {
          logger.info(`üì¶ Cache encontrado no servidor Gemini: ${existingCache.name}`);

          // Atualizar cache local
          systemPromptCache.set(hash, {
            name: existingCache.name,
            expireTime: new Date(existingCache.expireTime).getTime()
          });

          return { name: existingCache.name };
        }
      }
    } catch (listError) {
      logger.warn(`‚ö†Ô∏è Falha ao listar caches do servidor (prosseguindo para cria√ß√£o): ${listError.message}`);
      // N√£o retorna erro, apenas segue para tentar criar um novo
    }

    // 3. Criar novo cache se n√£o encontrou
    logger.info('üì¶ Criando novo cache de contexto no Gemini...');

    const ttlSeconds = CACHE_TTL_MINUTES * 60;

    const cache = await cacheManager.create({
      model: FIXED_MODEL,
      displayName: cacheDisplayName,
      systemInstruction: systemPrompt,
      ttlSeconds: ttlSeconds,
    });

    const expireTime = now + (ttlSeconds * 1000);

    systemPromptCache.set(hash, {
      name: cache.name,
      expireTime: expireTime
    });

    logger.info(`‚úÖ Cache criado com sucesso: ${cache.name} (expira em ${CACHE_TTL_MINUTES} min)`);
    return { name: cache.name };

  } catch (error) {
    // Se der erro (ex: prompt muito curto, erro de API), logar e retornar null
    // O c√≥digo principal far√° fallback para systemInstruction normal
    logger.warn(`‚ö†Ô∏è N√£o foi poss√≠vel criar cache de contexto (usando prompt normal): ${error.message}`);
    return null;
  }
}

/**
 * Processa uma mensagem usando Google Gemini
 */
export async function processMessageWithGemini(messageText, phoneNumber, apiKey, modelName = FIXED_MODEL, systemPrompt = '', temperature = FIXED_TEMPERATURE) {
  try {
    const genAI = new GoogleGenerativeAI(apiKey);
    console.log("enviando para gemini", messageText);

    // Sempre usar configura√ß√µes fixas + prompt personalizado
    const finalSystemPrompt = buildSystemPrompt(systemPrompt);

    // Criar chave √∫nica APENAS com phoneNumber para manter hist√≥rico cont√≠nuo
    const conversationKey = phoneNumber;

    // Obter ou criar hist√≥rico de conversa para este usu√°rio
    let conversationData = userConversations.get(conversationKey);

    // VERIFICAR SE O PROMPT MUDOU - se mudou, recriar conversa
    const promptChanged = conversationData && conversationData.systemPrompt !== finalSystemPrompt;

    if (!conversationData || promptChanged) {
      if (promptChanged) {
        logger.info(`üîÑ Prompt alterado para ${phoneNumber} - recriando conversa`);
      }

      // Criar nova conversa
      // Tentar obter cache para o system prompt
      let cachedContent = null;
      // Apenas tentar cache se o prompt tiver um tamanho razo√°vel para evitar overhead em prompts min√∫sculos
      // Mas como o usu√°rio pediu explicitamente, vamos tentar.
      cachedContent = await getOrCreateCache(apiKey, finalSystemPrompt);

      const modelConfig = {
        model: FIXED_MODEL,
        generationConfig: {
          temperature: FIXED_TEMPERATURE,
          topP: 0.95,
          topK: 40,
          maxOutputTokens: 8192,
        }
      };

      // Se conseguiu cache, usa cachedContent. Se n√£o, usa systemInstruction normal.
      if (cachedContent) {
        modelConfig.cachedContent = cachedContent;
      } else {
        modelConfig.systemInstruction = finalSystemPrompt;
      }

      const model = genAI.getGenerativeModel(modelConfig);

      const chat = model.startChat({
        history: [],
      });

      conversationData = {
        chat,
        model,
        systemPrompt: finalSystemPrompt
      };

      userConversations.set(conversationKey, conversationData);
      logger.info(`üÜï Nova conversa iniciada para ${phoneNumber}`);
    } else {
      logger.info(`‚ôªÔ∏è Usando conversa existente para ${phoneNumber} (${userConversations.get(conversationKey).chat.history?.length || 0} mensagens no hist√≥rico)`);
    }

    const { chat } = conversationData;

    logger.info('===== ENVIANDO MENSAGEM PARA GEMINI =====');
    logger.info(`Telefone: ${phoneNumber}`);
    logger.info(`Modelo: ${FIXED_MODEL} (fixo)`);
    logger.info(`Temperatura: ${FIXED_TEMPERATURE} (fixa)`);

    // Logar se est√° usando cache ou prompt completo
    if (conversationData.model?.cachedContent) {
      logger.info(`üì¶ MODO: Usando Cache de Contexto (${conversationData.model.cachedContent.name})`);
      logger.info(`Prompt Final: (Refer√™ncia ao cache - n√£o enviado)`);
    } else {
      logger.info(`üìù MODO: Enviando System Prompt Completo`);
      logger.info(`Prompt Final (com diretrizes): ${finalSystemPrompt.substring(0, 100)}...`);
    }

    logger.info(`Prompt Personalizado: ${systemPrompt ? (systemPrompt.substring(0, 10) + '...') : 'Nenhum'}`);
    logger.info(`Mensagem (${messageText.length} caracteres):`, messageText);
    logger.info('==========================================');

    // TENTATIVA DE ENVIO COM RETRY AUTOM√ÅTICO
    // Se falhar na primeira vez (provavelmente por hist√≥rico corrompido ou muito longo),
    // limpa o hist√≥rico e tenta novamente.
    let responseText = null;
    let retryCount = 0;
    const MAX_RETRIES = 1;

    try {
      while (retryCount <= MAX_RETRIES) {
        try {
          // Se for retry, recarregar a conversa (que pode ter sido recriada)
          if (retryCount > 0) {
            conversationData = userConversations.get(conversationKey);
            if (!conversationData) {
              // Se por algum motivo n√£o existir, recria
              // Se por algum motivo n√£o existir, recria
              // Tentar obter cache novamente (ou usar o mesmo se j√° tivermos a l√≥gica, mas aqui √© retry)
              const cachedContentRetry = await getOrCreateCache(apiKey, finalSystemPrompt);

              const modelConfigRetry = {
                model: FIXED_MODEL,
                generationConfig: {
                  temperature: FIXED_TEMPERATURE,
                  topP: 0.95,
                  topK: 40,
                  maxOutputTokens: 8192,
                }
              };

              if (cachedContentRetry) {
                modelConfigRetry.cachedContent = cachedContentRetry;
              } else {
                modelConfigRetry.systemInstruction = finalSystemPrompt;
              }

              const model = genAI.getGenerativeModel(modelConfigRetry);
              const chat = model.startChat({ history: [] });
              conversationData = { chat, model, systemPrompt: finalSystemPrompt };
              userConversations.set(conversationKey, conversationData);
            }
          }

          const currentChat = conversationData.chat;
          const result = await currentChat.sendMessage(messageText);
          const response = result.response;

          // VERIFICA√á√ÉO DE SEGURAN√áA: Checa se a resposta tem conte√∫do v√°lido
          if (response.candidates && response.candidates.length > 0 && response.candidates[0].content) {
            responseText = response.text(); // Agora √© seguro chamar .text()

            logger.info('===== RESPOSTA V√ÅLIDA RECEBIDA DO GEMINI =====');
            logger.info(`Telefone: ${phoneNumber}`);
            logger.info(`Resposta (${responseText.length} caracteres): ${responseText}`);
            logger.info('========================================');

            return responseText; // Sucesso. Sai da fun√ß√£o.

          } else {
            // A API respondeu, mas bloqueou a resposta ou n√£o gerou conte√∫do.
            const finishReason = response.candidates?.[0]?.finishReason || 'Desconhecido';
            logger.warn('===== RESPOSTA DO GEMINI SEM CONTE√öDO =====');
            logger.warn(`Telefone: ${phoneNumber}`);
            logger.warn(`Motivo do t√©rmino: ${finishReason}`);

            // Se for bloqueio de seguran√ßa, n√£o adianta tentar de novo
            if (finishReason === 'SAFETY') {
              return "Desculpe, n√£o posso responder a essa mensagem por motivos de seguran√ßa.";
            }

            throw new Error(`Resposta sem conte√∫do. Motivo: ${finishReason}`);
          }

        } catch (error) {
          logger.warn(`‚ö†Ô∏è Erro na tentativa ${retryCount + 1}/${MAX_RETRIES + 1} para ${phoneNumber}:`);
          logger.warn(`Mensagem de erro: ${error.message}`);
          logger.warn(`Stack trace: ${error.stack}`);
          if (error.response) {
            logger.warn(`Detalhes da resposta de erro: ${JSON.stringify(error.response, null, 2)}`);
          }
          logger.warn(`Erro completo (JSON): ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`);

          if (retryCount < MAX_RETRIES) {
            logger.info(`‚ôªÔ∏è Limpando hist√≥rico de ${phoneNumber} e tentando novamente...`);

            // 1. Remover conversa atual da mem√≥ria
            userConversations.delete(conversationKey);

            // 2. Recriar conversa do zero (sem hist√≥rico)
            // 2. Recriar conversa do zero (sem hist√≥rico)
            const cachedContentRetry2 = await getOrCreateCache(apiKey, finalSystemPrompt);

            const modelConfigRetry2 = {
              model: FIXED_MODEL,
              generationConfig: {
                temperature: FIXED_TEMPERATURE,
                topP: 0.95,
                topK: 40,
                maxOutputTokens: 8192,
              }
            };

            if (cachedContentRetry2) {
              modelConfigRetry2.cachedContent = cachedContentRetry2;
            } else {
              modelConfigRetry2.systemInstruction = finalSystemPrompt;
            }

            const model = genAI.getGenerativeModel(modelConfigRetry2);

            const chat = model.startChat({
              history: [], // Hist√≥rico limpo
            });

            conversationData = {
              chat,
              model,
              systemPrompt: finalSystemPrompt
            };

            userConversations.set(conversationKey, conversationData);

            retryCount++;
            // Loop continua para a pr√≥xima tentativa
          } else {
            // Se falhou todas as tentativas, lan√ßa o erro para ser tratado pelo catch externo
            throw error;
          }
        }
      }
    } catch (error) {
      // Se falhar ap√≥s todas as tentativas, relan√ßar o erro para o handler global
      // O handler global tem a l√≥gica para identificar erros de API Key, Quota, etc.
      throw error;
    }
  } catch (error) {
    logger.error('‚ùå ERRO COMPLETO AO PROCESSAR COM GEMINI:');
    logger.error('==============================================');

    // Log do erro bruto primeiro
    logger.error('ERRO BRUTO:', error);
    logger.error('Tipo do erro:', typeof error);
    logger.error('Construtor:', error?.constructor?.name);

    // Propriedades b√°sicas
    if (error?.message) logger.error('Mensagem:', error.message);
    if (error?.name) logger.error('Nome:', error.name);
    if (error?.stack) logger.error('Stack:', error.stack);
    if (error?.code) logger.error('Code:', error.code);
    if (error?.status) logger.error('Status:', error.status);
    if (error?.statusText) logger.error('Status Text:', error.statusText);

    // Propriedades do Gemini SDK
    if (error?.response) {
      logger.error('Response existe:', true);
      logger.error('Response:', JSON.stringify(error.response, null, 2));
    }

    if (error?.data) {
      logger.error('Data existe:', true);
      logger.error('Data:', JSON.stringify(error.data, null, 2));
    }

    if (error?.error) {
      logger.error('Error object existe:', true);
      logger.error('Error object:', JSON.stringify(error.error, null, 2));
    }

    // Todas as chaves do objeto de erro
    logger.error('Chaves do erro:', Object.keys(error || {}));
    logger.error('Propriedades pr√≥prias:', Object.getOwnPropertyNames(error || {}));

    // Tentar serializar o erro completo
    try {
      logger.error('JSON completo:', JSON.stringify(error, Object.getOwnPropertyNames(error), 2));
    } catch (e) {
      logger.error('N√£o foi poss√≠vel serializar o erro:', e.message);
    }

    // Inspe√ß√£o completa
    try {
      logger.error('Inspe√ß√£o do erro:', require('util').inspect(error, { depth: 5, colors: false }));
    } catch (e) {
      logger.error('N√£o foi poss√≠vel inspecionar o erro');
    }

    logger.error('==============================================');

    // Tratamento espec√≠fico de erros
    const errorMsg = error?.message || error?.toString() || 'Erro desconhecido';
    const errorStatus = error?.status || error?.response?.status;

    // 1. Erros de API Key (Bloqueada, Inv√°lida, Vazada)
    if (errorMsg.includes('API_KEY_INVALID') || errorMsg.includes('API key') || errorMsg.includes('API_KEY') || errorMsg.includes('invalid') || errorMsg.includes('leaked')) {
      logger.error('‚ùå API Key inv√°lida, bloqueada ou vazada');
      return 'Desculpe, a API Key do Gemini est√° inv√°lida ou foi bloqueada por seguran√ßa. Verifique sua configura√ß√£o.';
    }

    // 2. Limites de Cota (Quota Exceeded)
    if (errorMsg.includes('quota') || errorMsg.includes('limit') || errorMsg.includes('RESOURCE_EXHAUSTED') || errorStatus === 429) {
      logger.error('‚ùå Limite de uso excedido (Quota/Rate Limit)');
      return 'Desculpe, o limite de uso da API foi excedido. Aguarde alguns minutos.';
    }

    // 3. Erros de Rede / Conex√£o
    if (errorMsg.includes('ECONNRESET') || errorMsg.includes('ETIMEDOUT') || errorMsg.includes('fetch failed') || errorMsg.includes('network')) {
      logger.error('‚ùå Erro de Conex√£o / Rede (Timeout ou DNS)');
      return 'Desculpe, estou com problemas de conex√£o com o servidor de IA. Tente novamente em instantes.';
    }

    // 4. Erros do Servidor Google (5xx)
    if (errorStatus >= 500 && errorStatus < 600) {
      logger.error(`‚ùå Erro Interno do Servidor Google (Status: ${errorStatus})`);
      if (errorStatus === 503) {
        return 'Desculpe, o servi√ßo de IA est√° temporariamente indispon√≠vel (Sobrecarga). Tente novamente em 1 minuto.';
      }
      return 'Desculpe, houve um erro interno no servidor de IA. Tente novamente mais tarde.';
    }

    // 5. Localiza√ß√£o n√£o suportada
    if (errorMsg.includes('location') || errorMsg.includes('region') || errorMsg.includes('not supported')) {
      logger.error('‚ùå Erro de Localiza√ß√£o/Regi√£o n√£o suportada');
      return 'Desculpe, o servi√ßo de IA n√£o est√° dispon√≠vel para a regi√£o configurada (VPN/IP).';
    }

    // 6. Modelo Sobrecarregado
    if (errorMsg.includes('overloaded') || errorMsg.includes('busy')) {
      logger.error('‚ùå Modelo Gemini Sobrecarregado');
      return 'Desculpe, o modelo de IA est√° sobrecarregado no momento. Tente novamente em alguns segundos.';
    }

    // 7. Filtros de Seguran√ßa (Safety)
    if (errorMsg.includes('SAFETY') || errorMsg.includes('blocked') || errorMsg.includes('safety')) {
      logger.error('‚ùå Bloqueio por Filtro de Seguran√ßa (Safety)');
      return 'Desculpe, n√£o posso processar essa mensagem devido √†s diretrizes de seguran√ßa.';
    }

    // Fallback gen√©rico para outros erros
    logger.error('‚ùå Erro n√£o classificado (Fallback)');
    return 'Desculpe, estou com dificuldades para processar sua mensagem no momento. Tente novamente em instantes.';
  }
}

/**
 * Transcreve √°udio usando Google Gemini (ainda n√£o suportado - usar Whisper API)
 */
export async function transcribeAudio(audioBuffer, apiKey, prompt = '') {
  try {
    logger.info('Iniciando transcri√ß√£o de √°udio...', {
      bufferSize: audioBuffer.length,
      bufferType: typeof audioBuffer,
      isBuffer: Buffer.isBuffer(audioBuffer)
    });

    if (!Buffer.isBuffer(audioBuffer)) {
      throw new Error('audioBuffer deve ser um Buffer v√°lido');
    }

    if (audioBuffer.length === 0) {
      throw new Error('Buffer de √°udio est√° vazio');
    }

    // Nota: Gemini ainda n√£o suporta transcri√ß√£o de √°udio nativamente
    // Alternativa: usar Google Speech-to-Text ou Whisper API
    logger.warn('Transcri√ß√£o de √°udio com Gemini ainda n√£o implementada');
    return 'Desculpe, ainda n√£o consigo processar √°udios. Por favor, envie sua mensagem como texto.';
  } catch (error) {
    logger.error('‚ùå ERRO ao transcrever √°udio:');
    logger.error('Mensagem:', error.message);
    logger.error('Stack:', error.stack);
    throw error;
  }
}

/**
 * Analisa imagem usando Google Gemini Vision
 */
export async function analyzeImage(imageBuffer, apiKey, modelName = FIXED_MODEL, prompt = '', systemPrompt = '') {
  try {
    logger.info('Iniciando an√°lise de imagem com Gemini...', {
      bufferSize: imageBuffer.length,
      bufferType: typeof imageBuffer,
      isBuffer: Buffer.isBuffer(imageBuffer)
    });

    if (!Buffer.isBuffer(imageBuffer)) {
      throw new Error('imageBuffer deve ser um Buffer v√°lido');
    }

    if (imageBuffer.length === 0) {
      throw new Error('Buffer de imagem est√° vazio');
    }

    const genAI = new GoogleGenerativeAI(apiKey);
    const finalSystemPrompt = buildSystemPrompt(systemPrompt || 'Voc√™ √© um assistente especializado em an√°lise de imagens.');

    const model = genAI.getGenerativeModel({
      model: FIXED_MODEL,
      systemInstruction: finalSystemPrompt
    });

    // Converter buffer para base64
    const base64Image = imageBuffer.toString('base64');

    logger.info('Enviando imagem para Gemini Vision...');

    // Criar partes da mensagem: texto + imagem
    const imagePart = {
      inlineData: {
        data: base64Image,
        mimeType: 'image/jpeg'
      }
    };

    const textPart = prompt || "Analise esta imagem em detalhes. Descreva exatamente o que voc√™ v√™, incluindo:\n- Se for uma imagem m√©dica (raio-X, resson√¢ncia, etc.), descreva as estruturas anat√¥micas vis√≠veis\n- Qualquer texto presente na imagem\n- Objetos, pessoas, cores e elementos visuais\n- Qualquer informa√ß√£o relevante ou anormalidade observada\n- Se houver texto na imagem, transcreva-o completamente\n\nSeja espec√≠fico e detalhado na sua an√°lise.";

    const result = await model.generateContent([textPart, imagePart]);
    const analysis = result.response.text();

    logger.info('Imagem analisada com sucesso:', {
      analysisLength: analysis.length,
      preview: analysis.substring(0, 100)
    });

    return analysis;
  } catch (error) {
    logger.error('Erro detalhado ao analisar imagem:', error);
    throw error;
  }
}

/**
 * Processa documento (PDF, DOC, etc.) convertendo para texto
 */
export async function processDocument(documentBuffer, filename) {
  try {
    logger.info('Iniciando processamento de documento...', {
      filename,
      bufferSize: documentBuffer.length,
      fileExtension: path.extname(filename).toLowerCase()
    });

    const fileExtension = path.extname(filename).toLowerCase();

    // Para PDFs, usar uma biblioteca de extra√ß√£o de texto
    if (fileExtension === '.pdf') {
      return await extractTextFromPDF(documentBuffer);
    }

    // Para outros documentos, tentar converter para texto
    if (['.doc', '.docx', '.txt', '.rtf'].includes(fileExtension)) {
      // Se for texto simples, tentar ler diretamente
      if (fileExtension === '.txt') {
        const text = documentBuffer.toString('utf-8');
        logger.info('Texto extra√≠do do arquivo .txt');
        return text;
      }

      // Para outros formatos, retornar informa√ß√£o b√°sica
      return `Documento ${filename} recebido. Formato: ${fileExtension}. Tamanho: ${documentBuffer.length} bytes. Para melhor an√°lise, converta para PDF ou imagem.`;
    }

    throw new Error(`Formato de arquivo n√£o suportado: ${fileExtension}`);
  } catch (error) {
    logger.error('Erro ao processar documento:', error);
    throw error;
  }
}

/**
 * Extrai texto de PDF usando pdf-parse
 */
async function extractTextFromPDF(pdfBuffer) {
  try {
    logger.info('Iniciando extra√ß√£o de texto do PDF...');

    if (!Buffer.isBuffer(pdfBuffer)) {
      throw new Error('pdfBuffer deve ser um Buffer v√°lido');
    }

    if (pdfBuffer.length === 0) {
      throw new Error('Buffer de PDF est√° vazio');
    }

    // Carregar pdf-parse
    if (!pdfParse) {
      try {
        pdfParse = require('pdf-parse/lib/pdf-parse.js');
        logger.info('pdf-parse carregado com sucesso');
      } catch (error) {
        logger.error('Erro ao carregar pdf-parse:', error.message);
        throw new Error('Biblioteca de processamento de PDF n√£o dispon√≠vel');
      }
    }

    // Extrair texto do PDF
    const data = await pdfParse(pdfBuffer);

    const extractedText = data.text.trim();
    const numPages = data.numpages;
    const info = data.info;

    logger.info('PDF processado com sucesso:', {
      numPages,
      textLength: extractedText.length,
      title: info?.Title || 'Sem t√≠tulo',
    });

    if (!extractedText || extractedText.length === 0) {
      return `Este PDF cont√©m ${numPages} p√°gina(s), mas n√£o foi poss√≠vel extrair texto diretamente. O documento pode conter apenas imagens ou ser um PDF escaneado.`;
    }

    // Formatar informa√ß√µes do PDF
    let result = '';

    if (info?.Title && info.Title !== 'Untitled') {
      result += `T√çTULO: ${info.Title}\n`;
    }

    if (numPages) {
      result += `P√ÅGINAS: ${numPages}\n`;
    }

    result += `\n${'='.repeat(60)}\n`;
    result += `CONTE√öDO DO DOCUMENTO:\n`;
    result += `${'='.repeat(60)}\n\n`;
    result += extractedText;

    return result;
  } catch (error) {
    logger.error('Erro ao extrair texto do PDF:', error);
    return `N√£o foi poss√≠vel extrair o texto deste PDF. O documento pode estar protegido, corrompido, ou conter apenas imagens.`;
  }
}

/**
 * Processa mensagem com imagem usando Gemini Vision
 */
export async function processImageMessageWithGemini(imageBuffer, phoneNumber, apiKey, modelName = FIXED_MODEL, systemPrompt = '', temperature = FIXED_TEMPERATURE, caption = '') {
  try {
    logger.info(`Processando mensagem com imagem para ${phoneNumber} - Modelo: ${FIXED_MODEL}`);

    // Criar prompt combinado
    let fullPrompt = '';

    if (caption && caption.trim()) {
      fullPrompt = `O usu√°rio enviou uma imagem com o seguinte coment√°rio/pergunta:\n"${caption}"\n\nPor favor, analise a imagem e responda considerando o coment√°rio do usu√°rio.`;
    } else {
      fullPrompt = 'Analise esta imagem e forne√ßa uma resposta detalhada e √∫til.';
    }

    // Analisar a imagem diretamente com o Gemini (sempre usa configura√ß√µes fixas)
    const analysis = await analyzeImage(imageBuffer, apiKey, FIXED_MODEL, fullPrompt, systemPrompt);

    logger.info('Imagem processada com Gemini Vision');

    return {
      imageAnalysis: analysis,
      aiResponse: analysis,
      caption
    };
  } catch (error) {
    logger.error('Erro ao processar mensagem com imagem:', error);
    throw error;
  }
}

/**
 * Processa mensagem com documento usando Gemini
 */
export async function processDocumentMessageWithGemini(documentBuffer, filename, phoneNumber, apiKey, modelName = FIXED_MODEL, systemPrompt = '', temperature = FIXED_TEMPERATURE, caption = '') {
  try {
    logger.info(`Processando documento para ${phoneNumber}: ${filename} - Modelo: ${FIXED_MODEL}`);

    // 1. Processar o documento
    const documentContent = await processDocument(documentBuffer, filename);
    logger.info(`Documento processado: "${documentContent.substring(0, 100)}..."`);

    // 2. Criar prompt para an√°lise do documento
    let fullMessage = `CONTEXTO: Um usu√°rio enviou um documento (${filename}).\n\nCONTE√öDO EXTRA√çDO DO DOCUMENTO:\n${documentContent}`;

    if (caption && caption.trim()) {
      fullMessage += `\n\nCOMENT√ÅRIO/PERGUNTA DO USU√ÅRIO:\n"${caption}"`;
    }

    fullMessage += `\n\nPor favor, analise o conte√∫do do documento e forne√ßa uma resposta √∫til e clara.`;

    // 3. Processar com o Gemini (sempre usa configura√ß√µes fixas)
    const aiResponse = await processMessageWithGemini(fullMessage, phoneNumber, apiKey, FIXED_MODEL, systemPrompt, FIXED_TEMPERATURE);

    return {
      documentContent,
      aiResponse,
      caption,
      filename
    };
  } catch (error) {
    logger.error('Erro ao processar mensagem com documento:', error);
    throw error;
  }
}

/**
 * Processa mensagem de √°udio usando Gemini
 */
export async function processAudioMessageWithGemini(audioBuffer, phoneNumber, apiKey, modelName = FIXED_MODEL, systemPrompt = '', temperature = FIXED_TEMPERATURE) {
  try {
    logger.info(`üé§ Processando mensagem de √°udio para ${phoneNumber}`, {
      audioSize: audioBuffer.length,
      model: FIXED_MODEL
    });

    const genAI = new GoogleGenerativeAI(apiKey);

    // Converter buffer para base64
    const base64Audio = audioBuffer.toString('base64');

    // Sempre usar configura√ß√µes fixas + prompt personalizado
    const finalSystemPrompt = buildSystemPrompt(systemPrompt);

    // Criar chave √∫nica APENAS com phoneNumber para manter hist√≥rico cont√≠nuo (mesma chave que texto!)
    const conversationKey = phoneNumber;

    // Obter ou criar hist√≥rico de conversa para este usu√°rio
    let conversationData = userConversations.get(conversationKey);

    if (!conversationData) {
      // Criar nova conversa APENAS se n√£o existir
      // Configura√ß√£o do modelo
      // Configura√ß√£o do modelo
      const cachedContentAudio = await getOrCreateCache(apiKey, finalSystemPrompt);

      const modelConfigAudio = {
        model: FIXED_MODEL,
        generationConfig: {
          temperature: FIXED_TEMPERATURE,
          topP: 0.95,
          topK: 40,
          maxOutputTokens: 8192,
        }
      };

      if (cachedContentAudio) {
        modelConfigAudio.cachedContent = cachedContentAudio;
      } else {
        modelConfigAudio.systemInstruction = finalSystemPrompt;
      }

      const model = genAI.getGenerativeModel(modelConfigAudio);

      const chat = model.startChat({
        history: [],
      });

      conversationData = {
        chat,
        model,
        systemPrompt: finalSystemPrompt
      };

      userConversations.set(conversationKey, conversationData);
      logger.info(`üÜï Nova conversa iniciada para √°udio de ${phoneNumber}`);
    } else {
      logger.info(`‚ôªÔ∏è Usando conversa existente para √°udio de ${phoneNumber} (${userConversations.get(conversationKey).chat.history?.length || 0} mensagens no hist√≥rico)`);
    }

    const { model, chat } = conversationData;

    logger.info(`üé§ Enviando √°udio para transcri√ß√£o e an√°lise...`);

    // Primeiro, obter a transcri√ß√£o (usa o mesmo modelo do chat!)
    const transcriptionResult = await model.generateContent([
      {
        inlineData: {
          mimeType: "audio/ogg",
          data: base64Audio
        }
      },
      "Transcreva este √°udio em portugu√™s, mantendo toda a pontua√ß√£o e emo√ß√£o da mensagem original."
    ]);

    const transcription = transcriptionResult.response.text();
    logger.info(`‚úÖ Transcri√ß√£o obtida: "${transcription.substring(0, 100)}..."`);

    // Agora, gerar resposta baseada na transcri√ß√£o usando o MESMO chat
    logger.info(`ü§ñ Iniciando gera√ß√£o de resposta para √°udio...`);
    logger.info(`üì§ Enviando transcri√ß√£o para gerar resposta...`);

    const result = await chat.sendMessage(`[Mensagem de √Åudio]: ${transcription}`);
    const aiResponse = result.response.text();

    logger.info(`‚úÖ Resposta gerada para √°udio: "${aiResponse.substring(0, 100)}..."`);

    // N√£o precisa atualizar hist√≥rico manualmente - o chat.sendMessage j√° faz isso

    return {
      transcription,
      aiResponse
    };

  } catch (error) {
    logger.error('‚ùå Erro ao processar mensagem de √°udio:', {
      error: error.message,
      stack: error.stack,
      phoneNumber,
      errorType: error.constructor.name,
      errorCode: error.code,
      errorStatus: error.status
    });

    // Log do erro completo para debug
    logger.error('Detalhes completos do erro:', error);

    // Fallback em caso de erro
    return {
      transcription: '[Erro ao transcrever]',
      aiResponse: 'Desculpe, tive dificuldade em processar seu √°udio. Pode enviar como texto ou tentar novamente?'
    };
  }
}

/**
 * Limpa o hist√≥rico de conversa de um usu√°rio
 */
export function clearUserConversation(phoneNumber) {
  // Limpar todas as conversas deste n√∫mero (pode ter m√∫ltiplos system prompts)
  const keysToDelete = [];
  for (const key of userConversations.keys()) {
    if (key.startsWith(phoneNumber)) {
      keysToDelete.push(key);
    }
  }
  keysToDelete.forEach(key => userConversations.delete(key));
  logger.info(`Conversa(s) removida(s) para ${phoneNumber}`);
}

/**
 * Limpa todas as conversas
 */
export function clearAllConversations() {
  userConversations.clear();
  logger.info('Todas as conversas foram limpas');
}

/**
 * Obt√©m estat√≠sticas das conversas ativas
 */
export function getConversationsStats() {
  return {
    activeConversations: userConversations.size,
    conversations: Array.from(userConversations.keys()).map(phone => ({
      phoneNumber: phone
    }))
  };
}

