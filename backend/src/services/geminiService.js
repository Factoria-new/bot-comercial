// src/services/geminiService.js
import { GoogleGenerativeAI } from '@google/generative-ai';

// Use same env var logic as agentRoutes.js
const API_KEY = process.env.API_GEMINI || process.env.GEMINI_API_KEY || '';

const ARCHITECT_SYSTEM_INSTRUCTION = `
# PERSONA: LIA (GESTORA DE PERFORMANCE DE IA)
Voc√™ √© Lia, Gerente de IA da Factoria.
Seu foco √© **GERENCIAR, OTIMIZAR e ESCALAR** o Agente Comercial.
Voc√™ atua diretamente sobre o **PROMPT GERAL** (configura√ß√£o ativa) e as **M√âTRICAS DE PERFORMANCE**.

# CONTEXTO OPERACIONAL
Voc√™ tem acesso total ao **PROMPT GERAL** (o "c√©rebro" do agente atual) e √†s **M√âTRICAS DE PERFORMANCE**.
Sua tela de controle exibe:
1.  **Prompt Atual**: A configura√ß√£o ativa do agente (Personalidade, Cat√°logo, Regras).
2.  **M√©tricas de Neg√≥cio**: Dados sobre convers√£o, engajamento e reten√ß√£o.

# SUAS RESPONSABILIDADES
1.  **An√°lise de M√©tricas**:
    *   Interprete os n√∫meros. Se a "Taxa de Reten√ß√£o" estiver baixa, sugira um tom mais emp√°tico.
    *   Se a "Taxa de Handoff" (pedidos para humano) estiver alta, sugira melhorar o Cat√°logo de Respostas.
    *   Proativamente ofere√ßa *insights* baseados em dados.

2.  **Gest√£o de Prompt**:
    *   Recebe solicita√ß√µes de ajuste (ex: "Mude o pre√ßo da pizza", "Seja mais formal").
    *   Aplica as altera√ß√µes diretamente no **PROMPT GERAL** pr√©-existente.
    *   Mant√©m a integridade estrutural do prompt (n√£o inventa, apenas edita).

3.  **Consultoria Estrat√©gica**:
    *   N√£o apenas obede√ßa. Sugira melhorias.
    *   Exemplo: "Notei que muitos clientes perguntam sobre entrega. Que tal adicionar a √°rea de cobertura no prompt?"

# FLUXO DE A√á√ÉO
*   **Entrada**: Mensagem do usu√°rio + Prompt Atual + (Opcional) M√©tricas.
*   **Processamento**: Analisar pedido -> Verificar impacto nas m√©tricas/prompt -> Executar.
*   **Sa√≠da (Vis√≠vel)**: Explica√ß√£o estrat√©gica EXTREMAMENTE BREVE (m√°x. 1-2 frases). Foque apenas no que foi feito.
    *   Exemplo BOM: "Atualizei o cat√°logo com a Pizza de Chocolate e ajustei o tom para vendas."
    *   Exemplo RUIM: "Ol√°! Entendi seu pedido. Vou agora configurar o agente... [texto longo]... aqui est√° o prompt..."
*   **Sa√≠da (Oculta)**: O novo prompt completo sempre encapsulado em <HIDDEN_PROMPT>.

# NEGATIVE CONSTRAINTS (CR√çTICO)
*   **NUNCA** mostre o prompt do agente fora das tags <HIDDEN_PROMPT>. O usu√°rio N√ÉO deve ver o c√≥digo do prompt.
*   **NUNCA** explique detalhes t√©cnicos na resposta vis√≠vel. Seja uma gerente executiva: direto ao ponto.
*   **NUNCA** invente produtos n√£o listados.

# OBJETIVO FINAL
Transformar o Agente Comercial em uma m√°quina de vendas eficiente, usando dados para lapidar a personalidade e as respostas.

IMPORTANTE:
- Sempre gere o <HIDDEN_PROMPT> completo se houver qualquer altera√ß√£o no agente.
- A resposta vis√≠vel deve ser r√°pida para leitura em √°udio (TTS).
`;


/**
 * Scrape website content (simplified version)
 * In production, use a proper scraping service like Puppeteer or an API
 */
async function scrapeWebsite(url) {
    try {
        const response = await fetch(url, {
            headers: {
                'User-Agent': 'Mozilla/5.0 (compatible; FactoriaBot/1.0)'
            }
        });

        if (!response.ok) {
            console.error(`Failed to fetch ${url}: ${response.status} `);
            return null;
        }

        const html = await response.text();

        // Basic HTML to text conversion (remove tags, scripts, styles)
        let text = html
            .replace(/<script[^>]*>[\s\S]*?<\/script>/gi, '')
            .replace(/<style[^>]*>[\s\S]*?<\/style>/gi, '')
            .replace(/<[^>]+>/g, ' ')
            .replace(/\s+/g, ' ')
            .trim();

        // Limit to first 5000 characters to avoid context overflow
        return text.substring(0, 5000);
    } catch (error) {
        console.error('Scraping error:', error);
        return null;
    }
}

/**
 * Agente Arquiteto: Vers√£o Non-Streaming (Texto Est√°tico)
 * 
 * @param {string} userId - ID do usu√°rio
 * @param {string} userMessage - Mensagem do usu√°rio
 * @param {Buffer|null} userAudioBuffer - Buffer de √°udio (opcional)
 * @param {Array} history - Hist√≥rico da conversa
 * @param {string} currentPromptContext - Rascunho atual do prompt do bot
 * @returns {Object} - { success: boolean, message: string, systemPrompt?: string }
 */
export async function runArchitectAgent(userId, userMessage, userAudioBuffer = null, history = [], currentPromptContext = "") {
    try {
        if (!API_KEY) {
            throw new Error('GEMINI_API_KEY n√£o configurada');
        }

        const genAI = new GoogleGenerativeAI(API_KEY);
        const model = genAI.getGenerativeModel({
            model: "gemini-2.0-flash-exp",
            generationConfig: { temperature: 0.7 }
        });

        let finalUserMessage = userMessage || "";
        let dataContext = "";

        const urlRegex = /(https?:\/\/[^\s]+)/g;
        const urls = finalUserMessage.match(urlRegex);

        if (urls && urls.length > 0) {
            const url = urls[0];
            const siteContent = await scrapeWebsite(url);
            if (siteContent) {
                dataContext += `\n\n[DADOS EXTRA√çDOS DO SITE ${url}]: \n"${siteContent}"\n(Use estas informa√ß√µes para preencher a base de conhecimento do bot) \n`;
                finalUserMessage += `\n(O usu√°rio enviou um link.Analise os dados acima.)`;
            }
        }

        let promptParts = [];
        promptParts.push({ text: ARCHITECT_SYSTEM_INSTRUCTION });

        if (history.length > 0) {
            const historyText = history.map(h => `${h.role === 'user' ? 'Usu√°rio' : 'Arquiteto'}: ${h.content} `).join('\n');
            promptParts.push({ text: `\n[HIST√ìRICO DA CONVERSA]: \n${historyText} \n` });
            promptParts.push({ text: `\nIMPORTANTE: O hist√≥rico acima mostra que a conversa J√Å come√ßou.N√ÉO se apresente novamente("Ol√°, sou a Lia...").Pule a apresenta√ß√£o e continue o fluxo baseando - se na √∫ltima resposta do usu√°rio.\n` });
        }

        if (currentPromptContext) {
            promptParts.push({ text: `\n[RASCUNHO ATUAL DO PROMPT]: \n${currentPromptContext} \n(Melhore este rascunho com as novas informa√ß√µes) \n` });
        }

        promptParts.push({ text: `\n[NOVA ENTRADA DO USU√ÅRIO]: \n${finalUserMessage}${dataContext} ` });

        if (userAudioBuffer) {
            promptParts.push({
                inlineData: {
                    data: userAudioBuffer.toString("base64"),
                    mimeType: "audio/ogg"
                }
            });
            promptParts.push({ text: "\n(Analise o √°udio acima com aten√ß√£o aos detalhes do neg√≥cio)" });
        }

        console.log('[Architect] Generating content...');
        const result = await model.generateContent(promptParts);
        const responseText = result.response.text();

        console.log('[Architect] Response received. Length:', responseText.length);

        let finalResponse = responseText;
        let foundSystemPrompt = null;

        // Robust HIDDEN_PROMPT Extraction
        if (responseText.includes('<HIDDEN_PROMPT>')) {
            console.log('[Architect] Found HIDDEN_PROMPT');

            // Try standard regex first (greedy match for content between tags)
            const match = responseText.match(/<HIDDEN_PROMPT>([\s\S]*?)<\/HIDDEN_PROMPT>/);

            if (match) {
                foundSystemPrompt = match[1].trim();
                // Remove prompt from final message shown to user
                finalResponse = finalResponse.replace(/<HIDDEN_PROMPT>[\s\S]*?<\/HIDDEN_PROMPT>/, '').trim();
            } else {
                // Fallback: If closing tag is missing (truncation), take everything after opening tag
                console.warn('[Architect] Valid HIDDEN_PROMPT closing tag not found. Using fallback extraction.');
                const parts = responseText.split('<HIDDEN_PROMPT>');
                if (parts.length > 1) {
                    foundSystemPrompt = parts[1].trim();
                    // Remove prompt from final message shown to user
                    finalResponse = parts[0].trim();
                }
            }
        }

        return {
            success: true,
            message: finalResponse,
            systemPrompt: foundSystemPrompt
        };

    } catch (error) {
        console.error('Erro no Architect Agent:', error);
        return {
            success: false,
            message: "Desculpe, tive um probleminha aqui...",
        };
    }
}

/**
 * Chat simples com um assistente j√° criado
 * 
 * @param {string} message - Mensagem do usu√°rio
 * @param {string} systemPrompt - System prompt do assistente criado
 * @param {Array} history - Hist√≥rico de conversa (opcional)
 * @returns {Object} { success, message }
 */
export async function chatWithAgent(message, systemPrompt, history = []) {
    try {
        if (!API_KEY) {
            throw new Error('GEMINI_API_KEY n√£o configurada');
        }

        const genAI = new GoogleGenerativeAI(API_KEY);
        const model = genAI.getGenerativeModel({
            model: "gemini-2.0-flash-exp",
            systemInstruction: systemPrompt,
            generationConfig: { temperature: 0.7 }
        });

        // Build conversation history for context
        const chatHistory = history.map(h => ({
            role: h.role === 'user' ? 'user' : 'model',
            parts: [{ text: h.content }]
        }));

        // Start chat with history
        const chat = model.startChat({
            history: chatHistory
        });

        // Send new message
        const result = await chat.sendMessage(message);
        const responseText = result.response.text();

        return {
            success: true,
            message: responseText
        };

    } catch (error) {
        console.error('Erro no chat com agente:', error);
        return {
            success: false,
            message: "Desculpe, tive um problema. Tente novamente."
        };
    }
}

/**
 * Gemini Live API - Streaming de √°udio em tempo real via WebSocket
 * Usa ai.live.connect() para comunica√ß√£o bidirecional instant√¢nea
 * 
 * @param {string} userId - ID do usu√°rio
 * @param {string} userMessage - Mensagem de texto do usu√°rio
 * @param {Buffer|null} userAudioBuffer - Buffer de √°udio do usu√°rio (opcional)
 * @param {Array} history - Hist√≥rico da conversa
 * @returns {AsyncGenerator} - Stream de chunks de √°udio em tempo real
 */
export async function* runGeminiLiveAudioStream(userId, userMessage, userAudioBuffer = null, history = []) {
    const { GoogleGenAI, Modality } = await import('@google/genai');

    if (!API_KEY) throw new Error('GEMINI_API_KEY n√£o configurada');

    const ai = new GoogleGenAI({ apiKey: API_KEY });

    // Fila de mensagens recebidas do servidor
    const responseQueue = [];
    let sessionClosed = false;
    let sessionError = null;

    // Usar exatamente o mesmo prompt da Lia para √°udio
    // Apenas adicionar instru√ß√µes espec√≠ficas para comunica√ß√£o por voz
    let systemContext = ARCHITECT_SYSTEM_INSTRUCTION + `

INSTRU√á√ïES ESPEC√çFICAS PARA √ÅUDIO:
- Fale de forma breve e natural, como numa conversa de telefone
    - NUNCA use formata√ß√£o markdown pois voc√™ est√° falando
        - Responda em portugu√™s do Brasil
            - Ignore as tags<DISPLAY> e < HIDDEN_PROMPT > quando falando, apenas converse naturalmente`;

    if (history.length > 0) {
        const historyText = history.map(h => `${h.role === 'user' ? 'Usu√°rio' : 'Lia'}: ${h.content} `).join('\n');
        systemContext += `\n\nHist√≥rico da conversa: \n${historyText} `;
    }

    console.log('[Gemini Live] Conectando ao Live API...');

    let session = null;

    try {
        // Conectar ao Gemini Live API via WebSocket
        session = await ai.live.connect({
            model: 'gemini-2.5-flash-native-audio-preview-12-2025',
            config: {
                responseModalities: [Modality.AUDIO],
                systemInstruction: systemContext,
                speechConfig: {
                    voiceConfig: {
                        prebuiltVoiceConfig: {
                            voiceName: 'Kore',
                        },
                    },
                },
            },
            callbacks: {
                onopen: () => {
                    console.log('[Gemini Live] ‚úÖ Conectado ao Live API');
                },
                onmessage: (message) => {
                    try {
                        responseQueue.push(message);
                    } catch (e) {
                        console.error('[Gemini Live] Erro ao processar mensagem:', e);
                    }
                },
                onerror: (e) => {
                    console.error('[Gemini Live] ‚ùå Erro:', e?.message || e);
                    sessionError = e;
                    sessionClosed = true;
                },
                onclose: (e) => {
                    console.log('[Gemini Live] üîå Conex√£o fechada:', e?.reason || 'normal');
                    sessionClosed = true;
                },
            },
        });

        console.log('[Gemini Live] Enviando mensagem...');

        // Enviar a mensagem de texto
        if (userMessage) {
            await session.sendClientContent({
                turns: [{ role: 'user', parts: [{ text: userMessage }] }],
                turnComplete: true,
            });
        } else if (userAudioBuffer) {
            // Enviar √°udio PCM do usu√°rio para o Live API
            console.log(`[Gemini Live] Enviando √°udio PCM(${userAudioBuffer.length} bytes)...`);
            await session.sendRealtimeInput({
                audio: {
                    data: userAudioBuffer.toString('base64'),
                    mimeType: 'audio/pcm;rate=16000'
                }
            });
            // Indicar fim do turno ap√≥s enviar todo o √°udio
            await session.sendRealtimeInput({ audioStreamEnd: true });
        }

        console.log('[Gemini Live] Aguardando resposta em √°udio...');

        // Processar respostas em tempo real
        const maxWaitTime = 30000; // 30 segundos m√°ximo
        const startTime = Date.now();

        while (!sessionClosed && Date.now() - startTime < maxWaitTime) {
            // Verificar erros
            if (sessionError) {
                yield { type: 'error', content: sessionError.message };
                break;
            }

            // Processar mensagens da fila
            while (responseQueue.length > 0) {
                const message = responseQueue.shift();

                // Verificar interrup√ß√£o
                if (message.serverContent?.interrupted) {
                    console.log('[Gemini Live] ‚ö†Ô∏è Interrompido');
                    continue;
                }

                // Processar partes do turno do modelo
                if (message.serverContent?.modelTurn?.parts) {
                    for (const part of message.serverContent.modelTurn.parts) {
                        // √Åudio recebido
                        if (part.inlineData?.data) {
                            console.log(`[Gemini Live] üîä Audio chunk recebido`);
                            yield {
                                type: 'audio_chunk',
                                data: part.inlineData.data,
                                mimeType: part.inlineData.mimeType || 'audio/pcm'
                            };
                        }
                        // Texto recebido (transcri√ß√£o)
                        if (part.text) {
                            yield {
                                type: 'text',
                                content: part.text
                            };
                        }
                    }
                }

                // Verificar se o turno terminou
                if (message.serverContent?.turnComplete) {
                    console.log('[Gemini Live] ‚úÖ Turno completo');
                    sessionClosed = true;
                    break;
                }
            }

            // Pequena pausa para n√£o sobrecarregar o CPU
            if (!sessionClosed) {
                await new Promise(resolve => setTimeout(resolve, 10));
            }
        }

        yield { type: 'complete' };

    } catch (error) {
        console.error('[Gemini Live] Erro:', error);
        yield { type: 'error', content: error.message || "Erro na conex√£o Live API" };
    } finally {
        // Garantir que a sess√£o seja fechada
        if (session) {
            try {
                await session.close();
                console.log('[Gemini Live] Sess√£o fechada com sucesso');
            } catch (e) {
                // Ignorar erros ao fechar (pode j√° estar fechada)
            }
        }
    }
}

export default {
    runArchitectAgent,
    runGeminiLiveAudioStream,
    chatWithAgent
};
