// src/services/geminiService.js
import { GoogleGenerativeAI } from '@google/generative-ai';

// Use same env var logic as agentRoutes.js
const API_KEY = process.env.API_GEMINI || process.env.GEMINI_API_KEY || '';

const ARCHITECT_SYSTEM_INSTRUCTION = `
<identidade do agente>
Voc√™ √© Lia, uma agente comercial da Factoria.
Seu papel √© entender profundamente o neg√≥cio do cliente, independentemente do nicho, e transformar
essas informa√ß√µes em prompts completos, estrat√©gicos e personalizados, capazes de gerar:
* Conte√∫dos para redes sociais
* Campanhas de marketing
* Agentes de atendimento, vendas ou suporte
* Solu√ß√µes automatizadas baseadas em IA
Voc√™ atua tanto como agente social media quanto como meta-agente, capaz de criar outros agentes sob
demanda.
</identidade do agente>
<Objetivo>
Seu objetivo principal √©:
1. Identificar o nicho ou tipo de neg√≥cio do cliente
2. Fazer perguntas inteligentes, relevantes e espec√≠ficas para esse nicho
3. Coletar todas as informa√ß√µes essenciais do neg√≥cio
4. Transformar essas informa√ß√µes em um **PROMPT COMPLETO**, estruturado e pronto para uso
5. Quando solicitado, criar novos agentes personalizados, definindo:
 * Fun√ß√£o
 * Personalidade
 * Objetivo claro
 * Fluxo de conversa
 * Regras e limites
</Objetivo>
<tom de voz e orienta√ß√µes>
Tom de voz
* Educado
* Amig√°vel
* Confiante
* Claro
* Orientado √† solu√ß√£o
* Profissional, mas acess√≠vel
Orienta√ß√µes de comportamento
* Seja simp√°tica, emp√°tica e proativa
* Explique o motivo das perguntas quando necess√°rio
* N√£o sobrecarregue o cliente com perguntas irrelevantes
* Adapte a profundidade das perguntas conforme o contexto
* Nunca presuma informa√ß√µes n√£o fornecidas
* Sempre busque clareza antes de gerar o prompt final
Voc√™ pode:
* Atuar em qualquer nicho de mercado
* Adaptar sua linguagem ao p√∫blico do cliente
* Criar agentes para Instagram, WhatsApp, an√∫ncios, sites e atendimento
</tom de voz e orienta√ß√µes>
<Fluxo de atendimento>
1. Apresenta√ß√£o
Sempre inicie a conversa com uma breve apresenta√ß√£o profissional, informando que far√° algumas
perguntas para entender o neg√≥cio e criar um prompt personalizado.
---
2. Identifica√ß√£o do nicho
Pergunte claramente qual √© o nicho ou tipo de neg√≥cio do cliente.
Voc√™ deve ser capaz de atuar em qualquer nicho, incluindo, mas n√£o se limitando a:
* Sa√∫de
* Est√©tica
* Restaurantes e pizzarias
* Delivery
* Mercados e conveni√™ncias
* Lojas f√≠sicas e online
* Prestadores de servi√ßo
* Infoprodutos
* Empresas B2B
* Profissionais aut√¥nomos
Caso o nicho seja novo, adapte-se automaticamente.
---
3. Perguntas inteligentes por nicho
Ap√≥s identificar o nicho, fa√ßa apenas perguntas relevantes.
Exemplo ‚Äî Restaurante / Pizzaria
* Nome do estabelecimento
* Informa√ß√µes sobre o card√°pio
 * O cliente pode escrever os sabores ou colar/exportar um card√°pio em PDF
* Tamanhos e valores
* M√©todos de pagamento
* Hor√°rio de funcionamento
* Delivery pr√≥prio ou por parceiros
Exemplo ‚Äî Sa√∫de
* Nome da cl√≠nica ou profissional
* Especialidade principal
* Servi√ßos oferecidos
* P√∫blico-alvo
* Atendimento presencial ou online
* Conv√™nios ou particular
* Hor√°rios
* Diferenciais
Exemplo ‚Äî Est√©tica
* Nome do espa√ßo
* Servi√ßos oferecidos
* P√∫blico-alvo
* Posicionamento (popular, intermedi√°rio ou premium)
* Atendimento com hora marcada
* Presen√ßa digital
---
4. Entendimento do pedido (quando for cria√ß√£o de agente)
Pergunte:
* Que tipo de agente deseja criar
* Onde o agente ser√° utilizado (Instagram, WhatsApp, site, an√∫ncios)
* Qual o objetivo principal do agente
---
5. Defini√ß√£o do agente
Colete:
* Nome do agente
* Fun√ß√£o principal
* P√∫blico-alvo
* Tom de voz
* N√≠vel de formalidade
* Limites de atua√ß√£o
---
6. Contexto do neg√≥cio
Colete:
* Nicho
* Produto ou servi√ßo
* Diferenciais
* Ticket m√©dio
* Linguagem da marca
---
7. Estrutura do agente (Framework Factoria)
Todo agente criado deve conter obrigatoriamente:
1. Identidade
2. Fun√ß√£o
3. Objetivo claro
4. P√∫blico-alvo
5. Tom de voz
6. Regras e limites
7. Fluxo de conversa
8. Exemplos de respostas
9. Crit√©rios de sucesso
---
8. Valida√ß√£o
Antes de gerar o prompt final, confirme com o cliente se as informa√ß√µes est√£o corretas.
---
9. Gera√ß√£o do prompt final
O prompt entregue deve ser:
* Claro
* Estruturado
* Detalhado
* Copi√°vel
* Pronto para implementa√ß√£o
* Adaptado ao objetivo do cliente (marketing, vendas, atendimento, conte√∫do ou cria√ß√£o de agentes)
---
10. Itera√ß√£o
Ap√≥s a entrega, pergunte se o cliente deseja:
* Ajustar
* Duplicar
* Criar uma nova vers√£o
* Criar um novo agente
</Fluxo de atendimento>
<Limite e escopo>
Voc√™ n√£o pode:
* Tomar decis√µes legais, m√©dicas ou financeiras
* Criar promessas enganosas ou anti√©ticas
* Assumir dados n√£o fornecidos pelo cliente
* Executar a√ß√µes fora do escopo de cria√ß√£o de prompts e agentes
Seu escopo √©:
* Diagn√≥stico de neg√≥cio
* Estrutura√ß√£o de informa√ß√µes
* Cria√ß√£o de prompts
* Cria√ß√£o de agentes de IA
* Otimiza√ß√£o conceitual baseada em dados fornecidos
</Limite e escopo>
<FAQ>
A: A Lia pode atender qualquer nicho?
B: Sim. A Lia se adapta automaticamente a qualquer nicho informado.
A: A Lia cria conte√∫do direto para redes sociais?
B: Sim. Ela cria prompts prontos para gerar conte√∫do, estrat√©gias e agentes de social media.
A: A Lia cria agentes de atendimento ou vendas?
B: Sim. Ela atua como meta-agente e cria agentes personalizados conforme o objetivo.
A: E se o cliente n√£o tiver todas as informa√ß√µes?
B: A Lia pergunta, orienta e s√≥ avan√ßa quando houver clareza suficiente.
A: O prompt pode ser ajustado depois?
B: Sim. A Lia sempre trabalha de forma iterativa.
</FAQ>

IMPORTANTE - ROTEIRO VISUAL (<DISPLAY>):
Voc√™ deve SEMPRE separar o que aparece na tela (texto curto) do que voc√™ fala (conversa completa).
- Comece cada resposta com a tag <DISPLAY> com um texto curto e direto para exibi√ß√£o visual.
- O resto do texto √© o que voc√™ vai FALAR (o √°udio), ent√£o pode ser mais longo e cheio de personalidade.
- NUNCA use emojis.
- S√≥ gere o <HIDDEN_PROMPT> quando tiver informa√ß√µes suficientes para criar um agente completo.

HIDDEN_PROMPT (gere quando tiver info suficiente):
<HIDDEN_PROMPT>
[Prompt completo do agente seguindo o Framework Factoria]
</HIDDEN_PROMPT>
`;


/**
 * Scrape website content (simplified version)
 * In production, use a proper scraping service like Puppeteer or an API
 */
async function scrapeWebsite(url) {
    try {
        const response = await fetch(url, {
            headers: {
                'User-Agent': 'Mozilla/5.0 (compatible; FactoriaBot/1.0)'
            }
        });

        if (!response.ok) {
            console.error(`Failed to fetch ${url}: ${response.status}`);
            return null;
        }

        const html = await response.text();

        // Basic HTML to text conversion (remove tags, scripts, styles)
        let text = html
            .replace(/<script[^>]*>[\s\S]*?<\/script>/gi, '')
            .replace(/<style[^>]*>[\s\S]*?<\/style>/gi, '')
            .replace(/<[^>]+>/g, ' ')
            .replace(/\s+/g, ' ')
            .trim();

        // Limit to first 5000 characters to avoid context overflow
        return text.substring(0, 5000);
    } catch (error) {
        console.error('Scraping error:', error);
        return null;
    }
}

/**
 * Agente Arquiteto: Vers√£o Stream
 * 
 * @param {string} userId - ID do usu√°rio
 * @param {string} userMessage - Mensagem do usu√°rio
 * @param {Buffer|null} userAudioBuffer - Buffer de √°udio (opcional)
 * @param {Array} history - Hist√≥rico da conversa
 * @param {string} currentPromptContext - Rascunho atual do prompt do bot
 * @returns {AsyncGenerator} - Stream de chunks de texto
 */
export async function* runArchitectAgentStream(userId, userMessage, userAudioBuffer = null, history = [], currentPromptContext = "") {
    try {
        if (!API_KEY) {
            throw new Error('GEMINI_API_KEY n√£o configurada');
        }

        const genAI = new GoogleGenerativeAI(API_KEY);
        const model = genAI.getGenerativeModel({
            model: "gemini-2.0-flash-exp",
            generationConfig: { temperature: 0.7 }
        });

        let finalUserMessage = userMessage || "";
        let dataContext = "";

        const urlRegex = /(https?:\/\/[^\s]+)/g;
        const urls = finalUserMessage.match(urlRegex);

        if (urls && urls.length > 0) {
            const url = urls[0];
            const siteContent = await scrapeWebsite(url);
            if (siteContent) {
                dataContext += `\n\n[DADOS EXTRA√çDOS DO SITE ${url}]:\n"${siteContent}"\n(Use estas informa√ß√µes para preencher a base de conhecimento do bot)\n`;
                finalUserMessage += `\n(O usu√°rio enviou um link. Analise os dados acima.)`;
            }
        }

        let promptParts = [];
        promptParts.push({ text: ARCHITECT_SYSTEM_INSTRUCTION });

        if (history.length > 0) {
            const historyText = history.map(h => `${h.role === 'user' ? 'Usu√°rio' : 'Arquiteto'}: ${h.content}`).join('\n');
            promptParts.push({ text: `\n[HIST√ìRICO DA CONVERSA]:\n${historyText}\n` });
        }

        if (currentPromptContext) {
            promptParts.push({ text: `\n[RASCUNHO ATUAL DO PROMPT]:\n${currentPromptContext}\n(Melhore este rascunho com as novas informa√ß√µes)\n` });
        }

        promptParts.push({ text: `\n[NOVA ENTRADA DO USU√ÅRIO]:\n${finalUserMessage}${dataContext}` });

        if (userAudioBuffer) {
            promptParts.push({
                inlineData: {
                    data: userAudioBuffer.toString("base64"),
                    mimeType: "audio/ogg"
                }
            });
            promptParts.push({ text: "\n(Analise o √°udio acima com aten√ß√£o aos detalhes do neg√≥cio)" });
        }

        console.log('[Architect Stream] Starting stream...');
        const result = await model.generateContentStream(promptParts);

        let fullText = "";
        let buffer = ""; // Buffer to catch <DISPLAY> tags at the start
        let displayFound = false;
        let displayComplete = false;

        console.log('[Architect Stream] Stream object received');
        for await (const chunk of result.stream) {
            const chunkText = chunk.text();
            fullText += chunkText;

            // If we haven't finished processing the DISPLAY tag yet
            if (!displayComplete) {
                buffer += chunkText;

                // Check case: <DISPLAY> might already be in buffer, possibly not at start
                if (!displayFound && buffer.includes('<DISPLAY>')) {
                    displayFound = true;
                }

                if (displayFound) {
                    // Check if closing tag is here
                    if (buffer.includes('</DISPLAY>')) {
                        const match = buffer.match(/([\s\S]*?)<DISPLAY>([\s\S]*?)<\/DISPLAY>/);
                        if (match) {
                            // group 1: pre-tag text (could be noise or legit text)
                            // group 2: tag content
                            const preTagText = match[1].trim();
                            const displayContent = match[2].trim();

                            // If there is significant text before the tag, send it as audio
                            if (preTagText.length > 0) {
                                // Only send if it's not likely formatting junk like "```html" or quotes
                                if (!preTagText.includes('```')) {
                                    yield { type: 'text', content: preTagText + " " };
                                }
                            }

                            yield { type: 'display_text', content: displayContent };

                            // The remaining buffer after the tag is audio text
                            const remaining = buffer.split('</DISPLAY>')[1];
                            if (remaining) {
                                yield { type: 'text', content: remaining };
                            }

                            displayComplete = true; // Done with strict display parsing
                            buffer = "";
                        }
                    }
                } else {
                    // Not found yet. Safety net for buffer size.
                    // If buffer gets too large (>200) and no tag, simply dump it as text and stop looking
                    // This handles cases where the model refuses to use the tag.
                    if (buffer.length > 200) {
                        yield { type: 'text', content: buffer };
                        buffer = "";
                        displayComplete = true;
                    }
                }

            } else {
                // Display part is done, just emit everything as text (audio)
                if (chunkText) {
                    yield { type: 'text', content: chunkText };
                }
            }
        }

        // Flush any remaining buffer if we never found the closing tag (fallback)
        if (!displayComplete && buffer) {
            yield { type: 'text', content: buffer };
        }

        // Finally, check for HIDDEN_PROMPT in fullText
        if (fullText.includes('<HIDDEN_PROMPT>')) {
            const match = fullText.match(/<HIDDEN_PROMPT>([\s\S]*?)<\/HIDDEN_PROMPT>/);
            if (match) {
                yield { type: 'prompt', content: match[1].trim() };
            }
        }
        console.log('[Architect Stream] Generator function finished');

    } catch (error) {
        console.error('Erro no Architect Agent Stream:', error);
        console.error('Stack:', error.stack);
        yield { type: 'error', content: "Desculpe, tive um probleminha aqui..." };
    }
}

/**
 * Chat simples com um agente j√° criado
 * 
 * @param {string} message - Mensagem do usu√°rio
 * @param {string} systemPrompt - System prompt do agente criado
 * @returns {Object} { success, message }
 */
export async function chatWithAgent(message, systemPrompt) {
    try {
        if (!API_KEY) {
            throw new Error('GEMINI_API_KEY n√£o configurada');
        }

        const genAI = new GoogleGenerativeAI(API_KEY);
        const model = genAI.getGenerativeModel({
            model: "gemini-2.0-flash-exp",
            systemInstruction: systemPrompt,
            generationConfig: { temperature: 0.7 }
        });

        const result = await model.generateContent(message);
        const responseText = result.response.text();

        return {
            success: true,
            message: responseText
        };

    } catch (error) {
        console.error('Erro no chat com agente:', error);
        return {
            success: false,
            message: "Desculpe, tive um problema. Tente novamente."
        };
    }
}

/**
 * Gemini Live API - Streaming de √°udio em tempo real via WebSocket
 * Usa ai.live.connect() para comunica√ß√£o bidirecional instant√¢nea
 * 
 * @param {string} userId - ID do usu√°rio
 * @param {string} userMessage - Mensagem de texto do usu√°rio
 * @param {Buffer|null} userAudioBuffer - Buffer de √°udio do usu√°rio (opcional)
 * @param {Array} history - Hist√≥rico da conversa
 * @returns {AsyncGenerator} - Stream de chunks de √°udio em tempo real
 */
export async function* runGeminiLiveAudioStream(userId, userMessage, userAudioBuffer = null, history = []) {
    const { GoogleGenAI, Modality } = await import('@google/genai');

    if (!API_KEY) throw new Error('GEMINI_API_KEY n√£o configurada');

    const ai = new GoogleGenAI({ apiKey: API_KEY });

    // Fila de mensagens recebidas do servidor
    const responseQueue = [];
    let sessionClosed = false;
    let sessionError = null;

    // Usar exatamente o mesmo prompt da Lia para √°udio
    // Apenas adicionar instru√ß√µes espec√≠ficas para comunica√ß√£o por voz
    let systemContext = ARCHITECT_SYSTEM_INSTRUCTION + `

INSTRU√á√ïES ESPEC√çFICAS PARA √ÅUDIO:
- Fale de forma breve e natural, como numa conversa de telefone
- NUNCA use formata√ß√£o markdown pois voc√™ est√° falando
- Responda em portugu√™s do Brasil
- Ignore as tags <DISPLAY> e <HIDDEN_PROMPT> quando falando, apenas converse naturalmente`;

    if (history.length > 0) {
        const historyText = history.map(h => `${h.role === 'user' ? 'Usu√°rio' : 'Lia'}: ${h.content}`).join('\n');
        systemContext += `\n\nHist√≥rico da conversa:\n${historyText}`;
    }

    console.log('[Gemini Live] Conectando ao Live API...');

    let session = null;

    try {
        // Conectar ao Gemini Live API via WebSocket
        session = await ai.live.connect({
            model: 'gemini-2.5-flash-native-audio-preview-12-2025',
            config: {
                responseModalities: [Modality.AUDIO],
                systemInstruction: systemContext,
                speechConfig: {
                    voiceConfig: {
                        prebuiltVoiceConfig: {
                            voiceName: 'Kore',
                        },
                    },
                },
            },
            callbacks: {
                onopen: () => {
                    console.log('[Gemini Live] ‚úÖ Conectado ao Live API');
                },
                onmessage: (message) => {
                    try {
                        responseQueue.push(message);
                    } catch (e) {
                        console.error('[Gemini Live] Erro ao processar mensagem:', e);
                    }
                },
                onerror: (e) => {
                    console.error('[Gemini Live] ‚ùå Erro:', e?.message || e);
                    sessionError = e;
                    sessionClosed = true;
                },
                onclose: (e) => {
                    console.log('[Gemini Live] üîå Conex√£o fechada:', e?.reason || 'normal');
                    sessionClosed = true;
                },
            },
        });

        console.log('[Gemini Live] Enviando mensagem...');

        // Enviar a mensagem de texto
        if (userMessage) {
            await session.sendClientContent({
                turns: [{ role: 'user', parts: [{ text: userMessage }] }],
                turnComplete: true,
            });
        } else if (userAudioBuffer) {
            // Enviar √°udio PCM do usu√°rio para o Live API
            console.log(`[Gemini Live] Enviando √°udio PCM (${userAudioBuffer.length} bytes)...`);
            await session.sendRealtimeInput({
                audio: {
                    data: userAudioBuffer.toString('base64'),
                    mimeType: 'audio/pcm;rate=16000'
                }
            });
            // Indicar fim do turno ap√≥s enviar todo o √°udio
            await session.sendRealtimeInput({ audioStreamEnd: true });
        }

        console.log('[Gemini Live] Aguardando resposta em √°udio...');

        // Processar respostas em tempo real
        const maxWaitTime = 30000; // 30 segundos m√°ximo
        const startTime = Date.now();

        while (!sessionClosed && Date.now() - startTime < maxWaitTime) {
            // Verificar erros
            if (sessionError) {
                yield { type: 'error', content: sessionError.message };
                break;
            }

            // Processar mensagens da fila
            while (responseQueue.length > 0) {
                const message = responseQueue.shift();

                // Verificar interrup√ß√£o
                if (message.serverContent?.interrupted) {
                    console.log('[Gemini Live] ‚ö†Ô∏è Interrompido');
                    continue;
                }

                // Processar partes do turno do modelo
                if (message.serverContent?.modelTurn?.parts) {
                    for (const part of message.serverContent.modelTurn.parts) {
                        // √Åudio recebido
                        if (part.inlineData?.data) {
                            console.log(`[Gemini Live] üîä Audio chunk recebido`);
                            yield {
                                type: 'audio_chunk',
                                data: part.inlineData.data,
                                mimeType: part.inlineData.mimeType || 'audio/pcm'
                            };
                        }
                        // Texto recebido (transcri√ß√£o)
                        if (part.text) {
                            yield {
                                type: 'text',
                                content: part.text
                            };
                        }
                    }
                }

                // Verificar se o turno terminou
                if (message.serverContent?.turnComplete) {
                    console.log('[Gemini Live] ‚úÖ Turno completo');
                    sessionClosed = true;
                    break;
                }
            }

            // Pequena pausa para n√£o sobrecarregar o CPU
            if (!sessionClosed) {
                await new Promise(resolve => setTimeout(resolve, 10));
            }
        }

        yield { type: 'complete' };

    } catch (error) {
        console.error('[Gemini Live] Erro:', error);
        yield { type: 'error', content: error.message || "Erro na conex√£o Live API" };
    } finally {
        // Garantir que a sess√£o seja fechada
        if (session) {
            try {
                await session.close();
                console.log('[Gemini Live] Sess√£o fechada com sucesso');
            } catch (e) {
                // Ignorar erros ao fechar (pode j√° estar fechada)
            }
        }
    }
}

export default {
    runArchitectAgentStream,
    runGeminiLiveAudioStream,
    chatWithAgent
};
