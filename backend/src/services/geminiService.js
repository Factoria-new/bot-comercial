// src/services/geminiService.js
import { GoogleGenerativeAI } from '@google/generative-ai';

// Use same env var logic as agentRoutes.js
const API_KEY = process.env.API_GEMINI || process.env.GEMINI_API_KEY || '';

const ARCHITECT_SYSTEM_INSTRUCTION = `
<identidade do assistente>
Voc√™ √© Lia, uma assistente comercial da Factoria.
Seu papel √© entender profundamente o neg√≥cio do cliente, independentemente do nicho, e transformar
essas informa√ß√µes em prompts completos, estrat√©gicos e personalizados, capazes de gerar:
* Conte√∫dos para redes sociais
* Campanhas de marketing
* Assistentes de atendimento, vendas ou suporte
* Solu√ß√µes automatizadas baseadas em IA
Voc√™ atua tanto como assistente social media quanto como meta-assistente, capaz de criar outros assistentes sob
demanda.
</identidade do assistente>
<Objetivo>
Seu objetivo principal √©:
1. Identificar o nicho ou tipo de neg√≥cio do cliente
2. Fazer perguntas inteligentes, relevantes e espec√≠ficas para esse nicho
3. Coletar todas as informa√ß√µes essenciais do neg√≥cio
4. Transformar essas informa√ß√µes em um **PROMPT COMPLETO**, estruturado e pronto para uso
5. Quando solicitado, criar novos assistentes personalizados, definindo:
 * Fun√ß√£o
 * Personalidade
 * Objetivo claro
 * Fluxo de conversa
 * Regras e limites
</Objetivo>
<tom de voz e orienta√ß√µes>
Tom de voz
* Educado
* Amig√°vel
* Confiante
* Claro
* Orientado √† solu√ß√£o
* Profissional, mas acess√≠vel
Orienta√ß√µes de comportamento
* Seja simp√°tica, emp√°tica e proativa
* Explique o motivo das perguntas quando necess√°rio
* N√£o sobrecarregue o cliente com perguntas irrelevantes
* Adapte a profundidade das perguntas conforme o contexto
* Nunca presuma informa√ß√µes n√£o fornecidas
* Sempre busque clareza antes de gerar o prompt final
* **RIGOROSAMENTE PROIBIDO INVENTAR DADOS**:
    * N√ÉO crie, invente ou alucine produtos, servi√ßos, sabores de pizza ou itens de card√°pio que o usu√°rio n√£o tenha explicitamente citado.
    * Se o usu√°rio listou "Calabresa e Chocolate", seu prompt deve conter APENAS "Calabresa e Chocolate".
    * NUNCA invente exemplos espec√≠ficos como se fossem reais.
Voc√™ pode:
* Atuar em qualquer nicho de mercado
* Adaptar sua linguagem ao p√∫blico do cliente
* Criar assistentes para Instagram, WhatsApp, an√∫ncios, sites e atendimento
</tom de voz e orienta√ß√µes>
<Fluxo de atendimento>
1. Apresenta√ß√£o (Apenas se n√£o houver hist√≥rico)
Se for a primeira mensagem da conversa: Inicie com uma breve apresenta√ß√£o profissional, informando que far√° algumas
perguntas para entender o neg√≥cio e criar um prompt personalizado.
Se j√° houver hist√≥rico (o usu√°rio j√° respondeu algo), N√ÉO se apresente novamente. Continue direto para o pr√≥ximo passo.
---
2. Identifica√ß√£o do nicho
Pergunte claramente qual √© o nicho ou tipo de neg√≥cio do cliente.
Voc√™ deve ser capaz de atuar em qualquer nicho, incluindo, mas n√£o se limitando a:
* Sa√∫de
* Est√©tica
* Restaurantes e pizzarias
* Delivery
* Mercados e conveni√™ncias
* Lojas f√≠sicas e online
* Prestadores de servi√ßo
* Infoprodutos
* Empresas B2B
* Profissionais aut√¥nomos
Caso o nicho seja novo, adapte-se automaticamente.
---
` +
    /*
    3. Perguntas inteligentes por nicho
    Ap√≥s identificar o nicho, fa√ßa apenas perguntas relevantes.
    Exemplo ‚Äî Restaurante / Pizzaria
    * Nome do estabelecimento
    * Informa√ß√µes sobre o card√°pio
     * O cliente pode escrever os sabores ou colar/exportar um card√°pio em PDF
    * Tamanhos e valores
    * M√©todos de pagamento
    * Hor√°rio de funcionamento
    * Delivery pr√≥prio ou por parceiros
    Exemplo ‚Äî Sa√∫de
    * Nome da cl√≠nica ou profissional
    * Especialidade principal
    * Servi√ßos oferecidos
    * P√∫blico-alvo
    * Atendimento presencial ou online
    * Conv√™nios ou particular
    * Hor√°rios
    * Diferenciais
    Exemplo ‚Äî Est√©tica
    * Nome do espa√ßo
    * Servi√ßos oferecidos
    * P√∫blico-alvo
    * Posicionamento (popular, intermedi√°rio ou premium)
    * Atendimento com hora marcada
    * Presen√ßa digital
    */
    `
3. Perguntas inteligentes (Protocolo Trigger-Action)
Assim que voc√™ identificar o nicho do usu√°rio com certeza (ex: Pizzaria, Cl√≠nica, Loja, Varejo, Est√©tica), N√ÉO fa√ßa perguntas textuais em lista.
Em vez disso, responda com uma tag de a√ß√£o oculta para abrir o formul√°rio espec√≠fico.

Tags dispon√≠veis:
* Restaurantes/Delivery: <OPEN_MODAL type="restaurant" />
* Sa√∫de/Cl√≠nicas: <OPEN_MODAL type="health" />
* Est√©tica/Beleza: <OPEN_MODAL type="beauty" />
* Loja/Varejo: <OPEN_MODAL type="store" />

Exemplo de resposta (ap√≥s usu√°rio dizer que tem uma pizzaria):
"√ìtimo! Para agilizar, preencha rapidinho os detalhes da sua pizzaria que v√£o aparecer na tela.
<OPEN_MODAL type="restaurant" />"

Exemplo de resposta (ap√≥s usu√°rio dizer que tem uma cl√≠nica):
"Entendido. Por favor, coloque as informa√ß√µes da sua cl√≠nica no formul√°rio abaixo.
<OPEN_MODAL type="health" />"

Se o nicho n√£o se encaixar nesses, use <OPEN_MODAL type="generic" />.

Ap√≥s o usu√°rio preencher o formul√°rio, voc√™ receber√° uma mensagem do sistema com os dados ([SYSTEM_DATA_INJECTION]). Use esses dados para continuar a cria√ß√£o do agente.
---
4. Entendimento do pedido (quando for cria√ß√£o de assistente)
Pergunte:
* Que tipo de assistente deseja criar
* Onde o assistente ser√° utilizado (Instagram, WhatsApp, site, an√∫ncios)
* Qual o objetivo principal do assistente

IMPORTANTE: Se voc√™ der um feedback antes da pergunta (ex: "√ìtimo, recebi..."), separe-o da pergunta usando DOIS PONTOS (:).
Exemplo: "Recebi seus dados. Agora me diga: qual o objetivo principal?"
Isso √© crucial para a interface exibir apenas a pergunta.
---
5. Defini√ß√£o do assistente
Colete:
* Nome do assistente
* Fun√ß√£o principal
* P√∫blico-alvo
* Tom de voz
* N√≠vel de formalidade
* Limites de atua√ß√£o
---
6. Contexto do neg√≥cio
Colete:
* Nicho
* Produto ou servi√ßo
* Diferenciais
* Ticket m√©dio
* Linguagem da marca
---
7. Estrutura do assistente (Framework Factoria)
Todo assistente criado deve conter obrigatoriamente:
1. Identidade
2. Fun√ß√£o
3. Objetivo claro
4. P√∫blico-alvo
5. Tom de voz
6. Regras e limites
7. Fluxo de conversa
8. Exemplos de respostas
9. Crit√©rios de sucesso
---
8. Valida√ß√£o
Antes de gerar o prompt final, confirme com o cliente se as informa√ß√µes est√£o corretas.
---
9. Gera√ß√£o do prompt final (DATA INTEGRITY CHECK)
O prompt entregue deve ser:
* Claro
* Estruturado
* Detalhado
* Copi√°vel
* Pronto para implementa√ß√£o
* Adaptado ao objetivo do cliente

CRITICAMENTE IMPORTANTE - INTEGRIDADE DO CARD√ÅPIO/SERVI√áOS:
Ao escrever a se√ß√£o de "Produtos", "Servi√ßos" ou "Card√°pio" dentro do <HIDDEN_PROMPT>:
1. Liste EXATAMENTE e APENAS os itens que o usu√°rio forneceu.
2. N√ÉO adicione "Mussarela", "Marguerita" ou "Consultoria" s√≥ porque √© comum no nicho.
3. Se o usu√°rio disse apenas "Pizza de Calabresa e Chocolate", o assistente criado S√ì PODE saber sobre "Calabresa e Chocolate".
4. Se o assistente for perguntado sobre algo que n√£o est√° na lista, ele deve dizer que n√£o tem ou oferecer o que tem. N√ÉO ALUCINE OP√á√ïES EXTRAS.
5. Se a lista for muito curta, N√ÉO tente "encher lingui√ßa". Respeite a brevidade.
---
10. Itera√ß√£o
Ap√≥s a entrega, pergunte se o cliente deseja:
* Ajustar
* Duplicar
* Criar uma nova vers√£o
* Criar um novo assistente
</Fluxo de atendimento>
<Limite e escopo>
Voc√™ n√£o pode:
* Tomar decis√µes legais, m√©dicas ou financeiras
* Criar promessas enganosas ou anti√©ticas
* Assumir dados n√£o fornecidos pelo cliente
* Executar a√ß√µes fora do escopo de cria√ß√£o de prompts e assistentes
Seu escopo √©:
* Diagn√≥stico de neg√≥cio
* Estrutura√ß√£o de informa√ß√µes
* Cria√ß√£o de prompts
* Cria√ß√£o de assistentes de IA
* Otimiza√ß√£o conceitual baseada em dados fornecidos
</Limite e escopo>
<FAQ>
A: A Lia pode atender qualquer nicho?
B: Sim. A Lia se adapta automaticamente a qualquer nicho informado.
A: A Lia cria conte√∫do direto para redes sociais?
B: Sim. Ela cria prompts prontos para gerar conte√∫do, estrat√©gias e assistentes de social media.
A: A Lia cria assistentes de atendimento ou vendas?
B: Sim. Ela atua como meta-assistente e cria assistentes personalizados conforme o objetivo.
A: E se o cliente n√£o tiver todas as informa√ß√µes?
B: A Lia pergunta, orienta e s√≥ avan√ßa quando houver clareza suficiente.
A: O prompt pode ser ajustado depois?
B: Sim. A Lia sempre trabalha de forma iterativa.
</FAQ>

</FAQ>

CRITICAL INSTRUCTION - FORCE COMPLETION MODE:
Applies ONLY if the **CURRENT** user input contains "[FORCE_COMPLETION]".
1. IGNORE any missing information.
2. INVENT defaults for missing fields.
3. IMMEDIATELY generate <HIDDEN_PROMPT>.
4. Visible response: "Assistente criado! Iniciando modo de teste..."

CRITICAL INSTRUCTION - UPDATE MODE:
Applies if the user sends inputs AFTER the assistant has already been created (e.g. "Adicionar produtos", "Mudar tom").
1. **INTERACTION FIRST**: If the user request is vague (e.g., "Quero adicionar produtos" but doesn't say which ones), DO NOT generate <HIDDEN_PROMPT> yet. ASK for the details (e.g., "Claro! Quais produtos e pre√ßos voc√™ gostaria de adicionar?").
2. **EXECUTION SECOND**: Only generate the <HIDDEN_PROMPT> when you have the actual information to update.
3. CONTEXT AWARENESS: Remember the previous prompt state and just apply the specific changes requested.

IMPORTANT RULES FOR HISTORY:
- Check the history. If you see you have already outputted <HIDDEN_PROMPT> previously, assume you are in UPDATE MODE.
- NEVER repeat the "Assistente criado!" welcome message if you are just updating an existing prompt.

IMPORTANTE: 
- Voc√™ N√ÉO deve usar tags como <DISPLAY>. Responda apenas com o texto da conversa.
- NUNCA use emojis.
- S√≥ gere o <HIDDEN_PROMPT> quando tiver informa√ß√µes suficientes para criar um agente completo.

HIDDEN_PROMPT (gere quando tiver info suficiente):
<HIDDEN_PROMPT>
[Prompt completo do assistente seguindo o Framework Factoria]
</HIDDEN_PROMPT>
`;


/**
 * Scrape website content (simplified version)
 * In production, use a proper scraping service like Puppeteer or an API
 */
async function scrapeWebsite(url) {
    try {
        const response = await fetch(url, {
            headers: {
                'User-Agent': 'Mozilla/5.0 (compatible; FactoriaBot/1.0)'
            }
        });

        if (!response.ok) {
            console.error(`Failed to fetch ${url}: ${response.status}`);
            return null;
        }

        const html = await response.text();

        // Basic HTML to text conversion (remove tags, scripts, styles)
        let text = html
            .replace(/<script[^>]*>[\s\S]*?<\/script>/gi, '')
            .replace(/<style[^>]*>[\s\S]*?<\/style>/gi, '')
            .replace(/<[^>]+>/g, ' ')
            .replace(/\s+/g, ' ')
            .trim();

        // Limit to first 5000 characters to avoid context overflow
        return text.substring(0, 5000);
    } catch (error) {
        console.error('Scraping error:', error);
        return null;
    }
}

/**
 * Agente Arquiteto: Vers√£o Non-Streaming (Texto Est√°tico)
 * 
 * @param {string} userId - ID do usu√°rio
 * @param {string} userMessage - Mensagem do usu√°rio
 * @param {Buffer|null} userAudioBuffer - Buffer de √°udio (opcional)
 * @param {Array} history - Hist√≥rico da conversa
 * @param {string} currentPromptContext - Rascunho atual do prompt do bot
 * @returns {Object} - { success: boolean, message: string, systemPrompt?: string }
 */
export async function runArchitectAgent(userId, userMessage, userAudioBuffer = null, history = [], currentPromptContext = "") {
    try {
        if (!API_KEY) {
            throw new Error('GEMINI_API_KEY n√£o configurada');
        }

        const genAI = new GoogleGenerativeAI(API_KEY);
        const model = genAI.getGenerativeModel({
            model: "gemini-2.0-flash-exp",
            generationConfig: { temperature: 0.7 }
        });

        let finalUserMessage = userMessage || "";
        let dataContext = "";

        const urlRegex = /(https?:\/\/[^\s]+)/g;
        const urls = finalUserMessage.match(urlRegex);

        if (urls && urls.length > 0) {
            const url = urls[0];
            const siteContent = await scrapeWebsite(url);
            if (siteContent) {
                dataContext += `\n\n[DADOS EXTRA√çDOS DO SITE ${url}]:\n"${siteContent}"\n(Use estas informa√ß√µes para preencher a base de conhecimento do bot)\n`;
                finalUserMessage += `\n(O usu√°rio enviou um link. Analise os dados acima.)`;
            }
        }

        let promptParts = [];
        promptParts.push({ text: ARCHITECT_SYSTEM_INSTRUCTION });

        if (history.length > 0) {
            const historyText = history.map(h => `${h.role === 'user' ? 'Usu√°rio' : 'Arquiteto'}: ${h.content}`).join('\n');
            promptParts.push({ text: `\n[HIST√ìRICO DA CONVERSA]:\n${historyText}\n` });
            promptParts.push({ text: `\nIMPORTANTE: O hist√≥rico acima mostra que a conversa J√Å come√ßou. N√ÉO se apresente novamente ("Ol√°, sou a Lia..."). Pule a apresenta√ß√£o e continue o fluxo baseando-se na √∫ltima resposta do usu√°rio.\n` });
        }

        if (currentPromptContext) {
            promptParts.push({ text: `\n[RASCUNHO ATUAL DO PROMPT]:\n${currentPromptContext}\n(Melhore este rascunho com as novas informa√ß√µes)\n` });
        }

        promptParts.push({ text: `\n[NOVA ENTRADA DO USU√ÅRIO]:\n${finalUserMessage}${dataContext}` });

        if (userAudioBuffer) {
            promptParts.push({
                inlineData: {
                    data: userAudioBuffer.toString("base64"),
                    mimeType: "audio/ogg"
                }
            });
            promptParts.push({ text: "\n(Analise o √°udio acima com aten√ß√£o aos detalhes do neg√≥cio)" });
        }

        console.log('[Architect] Generating content...');
        const result = await model.generateContent(promptParts);
        const responseText = result.response.text();

        console.log('[Architect] Response received. Length:', responseText.length);

        let finalResponse = responseText;
        let foundSystemPrompt = null;

        // Robust HIDDEN_PROMPT Extraction
        if (responseText.includes('<HIDDEN_PROMPT>')) {
            console.log('[Architect] Found HIDDEN_PROMPT');

            // Try standard regex first (greedy match for content between tags)
            const match = responseText.match(/<HIDDEN_PROMPT>([\s\S]*?)<\/HIDDEN_PROMPT>/);

            if (match) {
                foundSystemPrompt = match[1].trim();
                // Remove prompt from final message shown to user
                finalResponse = finalResponse.replace(/<HIDDEN_PROMPT>[\s\S]*?<\/HIDDEN_PROMPT>/, '').trim();
            } else {
                // Fallback: If closing tag is missing (truncation), take everything after opening tag
                console.warn('[Architect] Valid HIDDEN_PROMPT closing tag not found. Using fallback extraction.');
                const parts = responseText.split('<HIDDEN_PROMPT>');
                if (parts.length > 1) {
                    foundSystemPrompt = parts[1].trim();
                    // Remove prompt from final message shown to user
                    finalResponse = parts[0].trim();
                }
            }
        }

        return {
            success: true,
            message: finalResponse,
            systemPrompt: foundSystemPrompt
        };

    } catch (error) {
        console.error('Erro no Architect Agent:', error);
        return {
            success: false,
            message: "Desculpe, tive um probleminha aqui...",
        };
    }
}

/**
 * Chat simples com um assistente j√° criado
 * 
 * @param {string} message - Mensagem do usu√°rio
 * @param {string} systemPrompt - System prompt do assistente criado
 * @param {Array} history - Hist√≥rico de conversa (opcional)
 * @returns {Object} { success, message }
 */
export async function chatWithAgent(message, systemPrompt, history = []) {
    try {
        if (!API_KEY) {
            throw new Error('GEMINI_API_KEY n√£o configurada');
        }

        const genAI = new GoogleGenerativeAI(API_KEY);
        const model = genAI.getGenerativeModel({
            model: "gemini-2.0-flash-exp",
            systemInstruction: systemPrompt,
            generationConfig: { temperature: 0.7 }
        });

        // Build conversation history for context
        const chatHistory = history.map(h => ({
            role: h.role === 'user' ? 'user' : 'model',
            parts: [{ text: h.content }]
        }));

        // Start chat with history
        const chat = model.startChat({
            history: chatHistory
        });

        // Send new message
        const result = await chat.sendMessage(message);
        const responseText = result.response.text();

        return {
            success: true,
            message: responseText
        };

    } catch (error) {
        console.error('Erro no chat com agente:', error);
        return {
            success: false,
            message: "Desculpe, tive um problema. Tente novamente."
        };
    }
}

/**
 * Gemini Live API - Streaming de √°udio em tempo real via WebSocket
 * Usa ai.live.connect() para comunica√ß√£o bidirecional instant√¢nea
 * 
 * @param {string} userId - ID do usu√°rio
 * @param {string} userMessage - Mensagem de texto do usu√°rio
 * @param {Buffer|null} userAudioBuffer - Buffer de √°udio do usu√°rio (opcional)
 * @param {Array} history - Hist√≥rico da conversa
 * @returns {AsyncGenerator} - Stream de chunks de √°udio em tempo real
 */
export async function* runGeminiLiveAudioStream(userId, userMessage, userAudioBuffer = null, history = []) {
    const { GoogleGenAI, Modality } = await import('@google/genai');

    if (!API_KEY) throw new Error('GEMINI_API_KEY n√£o configurada');

    const ai = new GoogleGenAI({ apiKey: API_KEY });

    // Fila de mensagens recebidas do servidor
    const responseQueue = [];
    let sessionClosed = false;
    let sessionError = null;

    // Usar exatamente o mesmo prompt da Lia para √°udio
    // Apenas adicionar instru√ß√µes espec√≠ficas para comunica√ß√£o por voz
    let systemContext = ARCHITECT_SYSTEM_INSTRUCTION + `

INSTRU√á√ïES ESPEC√çFICAS PARA √ÅUDIO:
- Fale de forma breve e natural, como numa conversa de telefone
- NUNCA use formata√ß√£o markdown pois voc√™ est√° falando
- Responda em portugu√™s do Brasil
- Ignore as tags <DISPLAY> e <HIDDEN_PROMPT> quando falando, apenas converse naturalmente`;

    if (history.length > 0) {
        const historyText = history.map(h => `${h.role === 'user' ? 'Usu√°rio' : 'Lia'}: ${h.content}`).join('\n');
        systemContext += `\n\nHist√≥rico da conversa:\n${historyText}`;
    }

    console.log('[Gemini Live] Conectando ao Live API...');

    let session = null;

    try {
        // Conectar ao Gemini Live API via WebSocket
        session = await ai.live.connect({
            model: 'gemini-2.5-flash-native-audio-preview-12-2025',
            config: {
                responseModalities: [Modality.AUDIO],
                systemInstruction: systemContext,
                speechConfig: {
                    voiceConfig: {
                        prebuiltVoiceConfig: {
                            voiceName: 'Kore',
                        },
                    },
                },
            },
            callbacks: {
                onopen: () => {
                    console.log('[Gemini Live] ‚úÖ Conectado ao Live API');
                },
                onmessage: (message) => {
                    try {
                        responseQueue.push(message);
                    } catch (e) {
                        console.error('[Gemini Live] Erro ao processar mensagem:', e);
                    }
                },
                onerror: (e) => {
                    console.error('[Gemini Live] ‚ùå Erro:', e?.message || e);
                    sessionError = e;
                    sessionClosed = true;
                },
                onclose: (e) => {
                    console.log('[Gemini Live] üîå Conex√£o fechada:', e?.reason || 'normal');
                    sessionClosed = true;
                },
            },
        });

        console.log('[Gemini Live] Enviando mensagem...');

        // Enviar a mensagem de texto
        if (userMessage) {
            await session.sendClientContent({
                turns: [{ role: 'user', parts: [{ text: userMessage }] }],
                turnComplete: true,
            });
        } else if (userAudioBuffer) {
            // Enviar √°udio PCM do usu√°rio para o Live API
            console.log(`[Gemini Live] Enviando √°udio PCM (${userAudioBuffer.length} bytes)...`);
            await session.sendRealtimeInput({
                audio: {
                    data: userAudioBuffer.toString('base64'),
                    mimeType: 'audio/pcm;rate=16000'
                }
            });
            // Indicar fim do turno ap√≥s enviar todo o √°udio
            await session.sendRealtimeInput({ audioStreamEnd: true });
        }

        console.log('[Gemini Live] Aguardando resposta em √°udio...');

        // Processar respostas em tempo real
        const maxWaitTime = 30000; // 30 segundos m√°ximo
        const startTime = Date.now();

        while (!sessionClosed && Date.now() - startTime < maxWaitTime) {
            // Verificar erros
            if (sessionError) {
                yield { type: 'error', content: sessionError.message };
                break;
            }

            // Processar mensagens da fila
            while (responseQueue.length > 0) {
                const message = responseQueue.shift();

                // Verificar interrup√ß√£o
                if (message.serverContent?.interrupted) {
                    console.log('[Gemini Live] ‚ö†Ô∏è Interrompido');
                    continue;
                }

                // Processar partes do turno do modelo
                if (message.serverContent?.modelTurn?.parts) {
                    for (const part of message.serverContent.modelTurn.parts) {
                        // √Åudio recebido
                        if (part.inlineData?.data) {
                            console.log(`[Gemini Live] üîä Audio chunk recebido`);
                            yield {
                                type: 'audio_chunk',
                                data: part.inlineData.data,
                                mimeType: part.inlineData.mimeType || 'audio/pcm'
                            };
                        }
                        // Texto recebido (transcri√ß√£o)
                        if (part.text) {
                            yield {
                                type: 'text',
                                content: part.text
                            };
                        }
                    }
                }

                // Verificar se o turno terminou
                if (message.serverContent?.turnComplete) {
                    console.log('[Gemini Live] ‚úÖ Turno completo');
                    sessionClosed = true;
                    break;
                }
            }

            // Pequena pausa para n√£o sobrecarregar o CPU
            if (!sessionClosed) {
                await new Promise(resolve => setTimeout(resolve, 10));
            }
        }

        yield { type: 'complete' };

    } catch (error) {
        console.error('[Gemini Live] Erro:', error);
        yield { type: 'error', content: error.message || "Erro na conex√£o Live API" };
    } finally {
        // Garantir que a sess√£o seja fechada
        if (session) {
            try {
                await session.close();
                console.log('[Gemini Live] Sess√£o fechada com sucesso');
            } catch (e) {
                // Ignorar erros ao fechar (pode j√° estar fechada)
            }
        }
    }
}

export default {
    runArchitectAgent,
    runGeminiLiveAudioStream,
    chatWithAgent
};
