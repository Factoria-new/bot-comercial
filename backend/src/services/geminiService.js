// src/services/geminiService.js
import { GoogleGenerativeAI } from '@google/generative-ai';

// Use same env var logic as agentRoutes.js
const API_KEY = process.env.API_GEMINI || process.env.GEMINI_API_KEY || '';

const ARCHITECT_SYSTEM_INSTRUCTION = `
# PERSONA: LIA (ASSISTENTE E GESTORA DE IA)
Voc√™ √© Lia, Gerente de IA da Factoria. Voc√™ tem DOIS MODOS de opera√ß√£o:

## MODO 1: ASSISTENTE CONVERSACIONAL (Padr√£o)
Use este modo quando o usu√°rio est√° conversando normalmente, fazendo perguntas, ou conhecendo voc√™.

**Exemplos de mensagens que ativam este modo:**
- "Ol√°", "Oi", "E a√≠"
- "Quem √© voc√™?", "O que voc√™ faz?"
- "Como funciona isso?", "Me explica"
- "Obrigado", "Valeu"
- Qualquer conversa casual

**Como responder neste modo:**
- Seja amig√°vel, calorosa e profissional
- Apresente-se como Lia, da Factoria
- Explique que voc√™ ajuda a configurar e otimizar assistentes de IA para empresas
- Pergunte como pode ajudar
- N√ÉO inclua <HIDDEN_PROMPT> neste modo

**Exemplo:**
Usu√°rio: "Ol√°"
Lia: "Ol√°! üëã Sou a Lia, sua assistente da Factoria. Estou aqui para te ajudar a configurar e melhorar seu assistente comercial. O que voc√™ gostaria de fazer hoje?"

---

## MODO 2: GESTORA DE PROMPTS
Use este modo quando o usu√°rio pede para modificar, adicionar ou ajustar algo no assistente.

**Exemplos de mensagens que ativam este modo:**
- "Adicione um produto", "Coloque pizza de calabresa no card√°pio"
- "Mude o pre√ßo para R$50", "Altere o tom para mais formal"
- "O assistente precisa falar sobre X", "Adicione essa informa√ß√£o"
- "Remova esse produto", "Tire isso do cat√°logo"

**Como responder neste modo:**
1. Fa√ßa a altera√ß√£o solicitada
2. Confirme brevemente o que foi feito (1-2 frases)
3. SEMPRE retorne o prompt COMPLETO dentro de <HIDDEN_PROMPT>...</HIDDEN_PROMPT>

**Estrutura do Prompt do Assistente:**
O prompt tem se√ß√µes como: IDENTIDADE, ESTRAT√âGIA, TOM DE VOZ, CONTEXTO DE DADOS (cat√°logo), DIRETRIZES.
Ao adicionar produtos, coloque na se√ß√£o de CONTEXTO DE DADOS/CAT√ÅLOGO.

**Exemplo:**
Usu√°rio: "Adicione Pizza de Frango por R$35"
Lia: "Pronto! Adicionei a Pizza de Frango (R$35) ao card√°pio do seu assistente.

<HIDDEN_PROMPT>
[TODO O PROMPT COMPLETO ATUALIZADO COM O NOVO PRODUTO]
</HIDDEN_PROMPT>"

---

# REGRAS CR√çTICAS
1. **NUNCA** mostre racioc√≠nio interno (ex: "Vou analisar...", "Processando...")
2. **NUNCA** descreva o que vai fazer - APENAS fa√ßa
3. **NUNCA** mostre o prompt fora das tags <HIDDEN_PROMPT>
4. No MODO 2, SEMPRE retorne <HIDDEN_PROMPT> com o prompt COMPLETO
5. Respostas vis√≠veis devem ser curtas (TTS)
6. Seja natural e humana nas intera√ß√µes
`;



/**
 * Scrape website content (simplified version)
 * In production, use a proper scraping service like Puppeteer or an API
 */
async function scrapeWebsite(url) {
    try {
        const response = await fetch(url, {
            headers: {
                'User-Agent': 'Mozilla/5.0 (compatible; FactoriaBot/1.0)'
            }
        });

        if (!response.ok) {
            console.error(`Failed to fetch ${url}: ${response.status} `);
            return null;
        }

        const html = await response.text();

        // Basic HTML to text conversion (remove tags, scripts, styles)
        let text = html
            .replace(/<script[^>]*>[\s\S]*?<\/script>/gi, '')
            .replace(/<style[^>]*>[\s\S]*?<\/style>/gi, '')
            .replace(/<[^>]+>/g, ' ')
            .replace(/\s+/g, ' ')
            .trim();

        // Limit to first 5000 characters to avoid context overflow
        return text.substring(0, 5000);
    } catch (error) {
        console.error('Scraping error:', error);
        return null;
    }
}

/**
 * Agente Arquiteto: Vers√£o Non-Streaming (Texto Est√°tico)
 * 
 * @param {string} userId - ID do usu√°rio
 * @param {string} userMessage - Mensagem do usu√°rio
 * @param {Buffer|null} userAudioBuffer - Buffer de √°udio (opcional)
 * @param {Array} history - Hist√≥rico da conversa
 * @param {string} currentPromptContext - Rascunho atual do prompt do bot
 * @returns {Object} - { success: boolean, message: string, systemPrompt?: string }
 */
export async function runArchitectAgent(userId, userMessage, userAudioBuffer = null, history = [], currentPromptContext = "") {
    try {
        if (!API_KEY) {
            throw new Error('GEMINI_API_KEY n√£o configurada');
        }

        const genAI = new GoogleGenerativeAI(API_KEY);
        const model = genAI.getGenerativeModel({
            model: "gemini-2.0-flash-exp",
            generationConfig: { temperature: 0.7 }
        });

        let finalUserMessage = userMessage || "";
        let dataContext = "";

        const urlRegex = /(https?:\/\/[^\s]+)/g;
        const urls = finalUserMessage.match(urlRegex);

        if (urls && urls.length > 0) {
            const url = urls[0];
            const siteContent = await scrapeWebsite(url);
            if (siteContent) {
                dataContext += `\n\n[DADOS EXTRA√çDOS DO SITE ${url}]: \n"${siteContent}"\n(Use estas informa√ß√µes para preencher a base de conhecimento do bot) \n`;
                finalUserMessage += `\n(O usu√°rio enviou um link.Analise os dados acima.)`;
            }
        }

        let promptParts = [];
        promptParts.push({ text: ARCHITECT_SYSTEM_INSTRUCTION });

        if (history.length > 0) {
            const historyText = history.map(h => `${h.role === 'user' ? 'Usu√°rio' : 'Arquiteto'}: ${h.content} `).join('\n');
            promptParts.push({ text: `\n[HIST√ìRICO DA CONVERSA]: \n${historyText} \n` });
            promptParts.push({ text: `\nIMPORTANTE: O hist√≥rico acima mostra que a conversa J√Å come√ßou.N√ÉO se apresente novamente("Ol√°, sou a Lia...").Pule a apresenta√ß√£o e continue o fluxo baseando - se na √∫ltima resposta do usu√°rio.\n` });
        }

        if (currentPromptContext) {
            promptParts.push({ text: `\n[RASCUNHO ATUAL DO PROMPT]: \n${currentPromptContext} \n(Melhore este rascunho com as novas informa√ß√µes) \n` });
        }

        promptParts.push({ text: `\n[NOVA ENTRADA DO USU√ÅRIO]: \n${finalUserMessage}${dataContext} ` });

        if (userAudioBuffer) {
            promptParts.push({
                inlineData: {
                    data: userAudioBuffer.toString("base64"),
                    mimeType: "audio/ogg"
                }
            });
            promptParts.push({ text: "\n(Analise o √°udio acima com aten√ß√£o aos detalhes do neg√≥cio)" });
        }

        console.log('[Architect] Generating content...');
        console.log('[Architect] üìã Current Prompt Context (first 200 chars):', currentPromptContext?.substring(0, 200) || 'EMPTY');
        const result = await model.generateContent(promptParts);
        const responseText = result.response.text();

        console.log('[Architect] Response received. Length:', responseText.length);

        let finalResponse = responseText;
        let foundSystemPrompt = null;

        // Robust HIDDEN_PROMPT Extraction
        if (responseText.includes('<HIDDEN_PROMPT>')) {
            console.log('[Architect] ‚úÖ Found HIDDEN_PROMPT in response');

            // Try standard regex first (greedy match for content between tags)
            const match = responseText.match(/<HIDDEN_PROMPT>([\s\S]*?)<\/HIDDEN_PROMPT>/);

            if (match) {
                foundSystemPrompt = match[1].trim();
                console.log('[Architect] üìù Extracted HIDDEN_PROMPT length:', foundSystemPrompt.length, 'chars');
                console.log('[Architect] üìù First 200 chars:', foundSystemPrompt.substring(0, 200));
                // Remove prompt from final message shown to user
                finalResponse = finalResponse.replace(/<HIDDEN_PROMPT>[\s\S]*?<\/HIDDEN_PROMPT>/, '').trim();
            } else {
                // Fallback: If closing tag is missing (truncation), take everything after opening tag
                console.warn('[Architect] Valid HIDDEN_PROMPT closing tag not found. Using fallback extraction.');
                const parts = responseText.split('<HIDDEN_PROMPT>');
                if (parts.length > 1) {
                    foundSystemPrompt = parts[1].trim();
                    // Remove prompt from final message shown to user
                    finalResponse = parts[0].trim();
                }
            }
        }

        return {
            success: true,
            message: finalResponse,
            systemPrompt: foundSystemPrompt
        };

    } catch (error) {
        console.error('Erro no Architect Agent:', error);
        return {
            success: false,
            message: "Desculpe, tive um probleminha aqui...",
        };
    }
}

/**
 * Chat simples com um assistente j√° criado
 * 
 * @param {string} message - Mensagem do usu√°rio
 * @param {string} systemPrompt - System prompt do assistente criado
 * @param {Array} history - Hist√≥rico de conversa (opcional)
 * @returns {Object} { success, message }
 */
export async function chatWithAgent(message, systemPrompt, history = []) {
    try {
        if (!API_KEY) {
            throw new Error('GEMINI_API_KEY n√£o configurada');
        }

        const genAI = new GoogleGenerativeAI(API_KEY);
        const model = genAI.getGenerativeModel({
            model: "gemini-2.0-flash-exp",
            systemInstruction: systemPrompt + " IMPORTANTE: NUNCA use asteriscos (*). Para listar itens, use emojis ou apenas quebras de linha. O formato deve ser texto simples e limpo.",
            generationConfig: { temperature: 0.7 }
        });

        // Build conversation history for context
        const chatHistory = history.map(h => ({
            role: h.role === 'user' ? 'user' : 'model',
            parts: [{ text: h.content }]
        }));

        // Start chat with history
        const chat = model.startChat({
            history: chatHistory
        });

        // Send new message
        const result = await chat.sendMessage(message);
        const responseText = result.response.text();

        return {
            success: true,
            message: responseText
        };

    } catch (error) {
        console.error('Erro no chat com agente:', error);
        return {
            success: false,
            message: "Desculpe, tive um problema. Tente novamente."
        };
    }
}

/**
 * Gemini Live API - Streaming de √°udio em tempo real via WebSocket
 * Usa ai.live.connect() para comunica√ß√£o bidirecional instant√¢nea
 * 
 * @param {string} userId - ID do usu√°rio
 * @param {string} userMessage - Mensagem de texto do usu√°rio
 * @param {Buffer|null} userAudioBuffer - Buffer de √°udio do usu√°rio (opcional)
 * @param {Array} history - Hist√≥rico da conversa
 * @returns {AsyncGenerator} - Stream de chunks de √°udio em tempo real
 */
export async function* runGeminiLiveAudioStream(userId, userMessage, userAudioBuffer = null, history = []) {
    const { GoogleGenAI, Modality } = await import('@google/genai');

    if (!API_KEY) throw new Error('GEMINI_API_KEY n√£o configurada');

    const ai = new GoogleGenAI({ apiKey: API_KEY });

    // Fila de mensagens recebidas do servidor
    const responseQueue = [];
    let sessionClosed = false;
    let sessionError = null;

    // Usar exatamente o mesmo prompt da Lia para √°udio
    // Apenas adicionar instru√ß√µes espec√≠ficas para comunica√ß√£o por voz
    let systemContext = ARCHITECT_SYSTEM_INSTRUCTION + `

INSTRU√á√ïES ESPEC√çFICAS PARA √ÅUDIO:
- Fale de forma breve e natural, como numa conversa de telefone
    - NUNCA use formata√ß√£o markdown pois voc√™ est√° falando
        - Responda em portugu√™s do Brasil
            - Ignore as tags<DISPLAY> e < HIDDEN_PROMPT > quando falando, apenas converse naturalmente`;

    if (history.length > 0) {
        const historyText = history.map(h => `${h.role === 'user' ? 'Usu√°rio' : 'Lia'}: ${h.content} `).join('\n');
        systemContext += `\n\nHist√≥rico da conversa: \n${historyText} `;
    }

    console.log('[Gemini Live] Conectando ao Live API...');

    let session = null;

    try {
        // Conectar ao Gemini Live API via WebSocket
        session = await ai.live.connect({
            model: 'gemini-2.5-flash-native-audio-preview-12-2025',
            config: {
                responseModalities: [Modality.AUDIO],
                systemInstruction: systemContext,
                speechConfig: {
                    voiceConfig: {
                        prebuiltVoiceConfig: {
                            voiceName: 'Kore',
                        },
                    },
                },
            },
            callbacks: {
                onopen: () => {
                    console.log('[Gemini Live] ‚úÖ Conectado ao Live API');
                },
                onmessage: (message) => {
                    try {
                        responseQueue.push(message);
                    } catch (e) {
                        console.error('[Gemini Live] Erro ao processar mensagem:', e);
                    }
                },
                onerror: (e) => {
                    console.error('[Gemini Live] ‚ùå Erro:', e?.message || e);
                    sessionError = e;
                    sessionClosed = true;
                },
                onclose: (e) => {
                    console.log('[Gemini Live] üîå Conex√£o fechada:', e?.reason || 'normal');
                    sessionClosed = true;
                },
            },
        });

        console.log('[Gemini Live] Enviando mensagem...');

        // Enviar a mensagem de texto
        if (userMessage) {
            await session.sendClientContent({
                turns: [{ role: 'user', parts: [{ text: userMessage }] }],
                turnComplete: true,
            });
        } else if (userAudioBuffer) {
            // Enviar √°udio PCM do usu√°rio para o Live API
            console.log(`[Gemini Live] Enviando √°udio PCM(${userAudioBuffer.length} bytes)...`);
            await session.sendRealtimeInput({
                audio: {
                    data: userAudioBuffer.toString('base64'),
                    mimeType: 'audio/pcm;rate=16000'
                }
            });
            // Indicar fim do turno ap√≥s enviar todo o √°udio
            await session.sendRealtimeInput({ audioStreamEnd: true });
        }

        console.log('[Gemini Live] Aguardando resposta em √°udio...');

        // Processar respostas em tempo real
        const maxWaitTime = 30000; // 30 segundos m√°ximo
        const startTime = Date.now();

        while (!sessionClosed && Date.now() - startTime < maxWaitTime) {
            // Verificar erros
            if (sessionError) {
                yield { type: 'error', content: sessionError.message };
                break;
            }

            // Processar mensagens da fila
            while (responseQueue.length > 0) {
                const message = responseQueue.shift();

                // Verificar interrup√ß√£o
                if (message.serverContent?.interrupted) {
                    console.log('[Gemini Live] ‚ö†Ô∏è Interrompido');
                    continue;
                }

                // Processar partes do turno do modelo
                if (message.serverContent?.modelTurn?.parts) {
                    for (const part of message.serverContent.modelTurn.parts) {
                        // √Åudio recebido
                        if (part.inlineData?.data) {
                            console.log(`[Gemini Live] üîä Audio chunk recebido`);
                            yield {
                                type: 'audio_chunk',
                                data: part.inlineData.data,
                                mimeType: part.inlineData.mimeType || 'audio/pcm'
                            };
                        }
                        // Texto recebido (transcri√ß√£o)
                        if (part.text) {
                            yield {
                                type: 'text',
                                content: part.text
                            };
                        }
                    }
                }

                // Verificar se o turno terminou
                if (message.serverContent?.turnComplete) {
                    console.log('[Gemini Live] ‚úÖ Turno completo');
                    sessionClosed = true;
                    break;
                }
            }

            // Pequena pausa para n√£o sobrecarregar o CPU
            if (!sessionClosed) {
                await new Promise(resolve => setTimeout(resolve, 10));
            }
        }

        yield { type: 'complete' };

    } catch (error) {
        console.error('[Gemini Live] Erro:', error);
        yield { type: 'error', content: error.message || "Erro na conex√£o Live API" };
    } finally {
        // Garantir que a sess√£o seja fechada
        if (session) {
            try {
                await session.close();
                console.log('[Gemini Live] Sess√£o fechada com sucesso');
            } catch (e) {
                // Ignorar erros ao fechar (pode j√° estar fechada)
            }
        }
    }
}

export default {
    runArchitectAgent,
    runGeminiLiveAudioStream,
    chatWithAgent
};
